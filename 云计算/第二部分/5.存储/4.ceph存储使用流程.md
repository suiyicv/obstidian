# 一.ceph三种存储接口

文件系统存储  cephFS, 依赖于MDS  
块存储             RBD  
对象存储         RGW

# 二.文件系统存储

## 1. 在ceph集群中部署MDS
ceph-deploy mds create node01 node02 node03
![[Pasted image 20240618145442.png]]
netstat -tunlp | grep mds
![[Pasted image 20240618145601.png]]
## 2.创建存储池
一个文件系统存储需要两个RADOS存储池，一个用于存储实体数据，一个用于存储元数据,上面分别创建两个存储池，名称为db_data, db_metadata  
分别指定存储池对应的PG数量为128， 64
存储池对应PG数量参考
少于5个OSD则PG数为128
5-10个OSD则PG数为512
10-50个OSD则PG数为1024
如果有更多的OSD需要自己理解计算
PG计算公式
Total PGs = ((Total_number_of_OSD * 100) / max_replication_count) / pool_coun

ceph osd pool create db_data 128          # 存储实际数据
ceph osd pool create db_metadata 64   # 存储元数据信息
![[Pasted image 20240618150605.png|625]]
ceph -s
![[Pasted image 20240618150706.png]]
ceph osd pool ls
![[Pasted image 20240618150744.png|625]]

## 3.创建文件系统存储
ceph fs new mydata db_metadata db_data
![[Pasted image 20240618153608.png]]
ceph fs ls
![[Pasted image 20240618153623.png]]
## 4.业务服务器挂载使用cephfs
ceph集群默认启用了cephx的认证，业务服务器要挂载使用ceph需要通过令牌认证
### 4.1 将认证的令牌导出，拷贝到业务服务器
cat ceph.client.admin.keyring
ceph-authtool -p ceph.client.admin.keyring > /root/client.keyring
scp /root/client.keyring root@192.168.137.14:/root/
### 4.2 业务服务器挂载使用ceph
yum install -y ceph-fuse
mkdir /test1
mount -t ceph node01:6789:/ /test1 -o name=admin,secretfile=/root/client.keyring
df -hT
![[Pasted image 20240618154516.png]]
永久挂载
vim /etc/fstab
node01:6789 /test1 ceph defaults,name=admin,secretfile=/root/client.keyring 0 0 

## 5.删除文件系统存储

### 5.1 业务服务器取消挂载
umount /test1
### 5.2 修改ceph.conf
添加允许删除的配置；同步配置文件
vim ceph.conf
	mon_allow_pool_delete = true 
ceph-deploy --overwrite-conf admin node01 node02 node03
systemctl restart ceph-mon.target

### 5.3 停掉所有集群节点的mds服务
systemctl stop ceph-mds.target

### 5.4 删除文件系统
ceph fs ls
ceph fs rm mydata --yes-i-really-mean-it
ceph fs ls
### 5.5 删除文件系统对应的存储池
ceph osd pool ls
ceph osd pool delete db_metadata db_metadata --yes-i-really-really-mean-it
ceph osd pool delete db_data db_data --yes-i-really-really-mean-it

# 三.块存储的使用
## 1.将ceph的配置同步到业务服务器
ceph-deploy admin app
![[Pasted image 20240618234246.png]]
后续的操作均在业务服务器上执行
## 2.创建存储池、初始化
ceph osd pool create block_pool 128
![[Pasted image 20240618234350.png]]
rbd pool init block_pool
## 3.创建卷
rbd create db_volume --pool block_pool --size 5000
rbd ls block_pool
![[Pasted image 20240618234444.png]]
## 4.映射块设备
rbd map block_pool/db_volume
![[Pasted image 20240618234642.png]]
rbd feature disable block_pool/db_volume object-map fast-diff deep-flatten
rbd map block_pool/db_volume
![[Pasted image 20240618234727.png]]
lsblk

## 5.使用ceph的块设备存储数据

## 6.扩容
## 7.缩容
## 8.删除块存储

# 四.对象存储
### 1、创建rgw服务
### 2、安装s3cmd测试工具
### 3、生成连接对象存储需要的AK、SK
### 4、创建.s3cfg配置文件，指定对象存储网关的连接信息
### 5、创建桶
### 6、测试文件上传、下载
