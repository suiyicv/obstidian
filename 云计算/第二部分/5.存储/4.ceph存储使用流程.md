# 一.ceph三种存储接口

文件系统存储  cephFS, 依赖于MDS  
块存储             RBD  
对象存储         RGW

# 二.文件系统存储

## 1. 在ceph集群中部署MDS
ceph-deploy mds create node01 node02 node03
![[Pasted image 20240618145442.png]]
netstat -tunlp | grep mds
![[Pasted image 20240618145601.png]]
## 2.创建存储池
一个文件系统存储需要两个RADOS存储池，一个用于存储实体数据，一个用于存储元数据,上面分别创建两个存储池，名称为db_data, db_metadata  
分别指定存储池对应的PG数量为128， 64
存储池对应PG数量参考
少于5个OSD则PG数为128
5-10个OSD则PG数为512
10-50个OSD则PG数为1024
如果有更多的OSD需要自己理解计算
PG计算公式
Total PGs = ((Total_number_of_OSD * 100) / max_replication_count) / pool_coun

ceph osd pool create db_data 128          # 存储实际数据
ceph osd pool create db_metadata 64   # 存储元数据信息
![[Pasted image 20240618150605.png|625]]
ceph -s
![[Pasted image 20240618150706.png]]
ceph osd pool ls
![[Pasted image 20240618150744.png|625]]

## 3.创建文件系统存储
ceph fs new mydata db_metadata db_data
![[Pasted image 20240618153608.png]]
ceph fs ls
![[Pasted image 20240618153623.png]]
## 4.业务服务器挂载使用cephfs
ceph集群默认启用了cephx的认证，业务服务器要挂载使用ceph需要通过令牌认证
### 4.1 将认证的令牌导出，拷贝到业务服务器
cat ceph.client.admin.keyring
ceph-authtool -p ceph.client.admin.keyring > /root/client.keyring
scp /root/client.keyring root@192.168.137.14:/root/
### 4.2 业务服务器挂载使用ceph
yum install -y ceph-fuse
mkdir /test1
mount -t ceph node01:6789:/ /test1 -o name=admin,secretfile=/root/client.keyring
df -hT
![[Pasted image 20240618154516.png]]
## 5.删除文件系统存储

### 5.1 业务服务器取消挂载
### 5.2 修改ceph.conf
添加允许删除的配置；同步配置文件

### 5.3 停掉所有集群节点的mds服务

### 5.4 删除文件系统
### 5.5 删除文件系统对应的存储池

# 三.块存储的使用