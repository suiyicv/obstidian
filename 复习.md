# 一.linux基础
Linux操作系统的内核，叫做<span style="background:#affad1">kernel</span>，如果希望找到它的最新源代码，我们介绍过了网站：kernel.org。kernel提供操作系统的最基本的功能，驱动硬件、初始化内存、CPU等。
什么是<span style="background:#affad1">shell</span>
Shell是系统的用户界面，提供了用户与内核进行交互操作的一种接口(命令解释器)
Shell接收用户输入的命令并把它送入内核去执行
Shell起着协调用户与系统的一致性和在用户与系统之间进行交互的作用
![[Excalidraw/Drawing 2024-07-10 19.49.48.excalidraw.md#^group=J6ZFZ8fHNER5oMS4pwtCd]]
## 0. 快捷键 
ctrl+alt + F1 图形 F2-F6文本
![[Pasted image 20240709200450.png|500]]
`CTRL+C`：发送 `SIGINT` 信号，通常用于中断并终止一个进程。
`CTRL+Z`：发送 `SIGTSTP` 信号，用于暂停（挂起）一个进程，但不终止它。

**查看别名**
```bash

alias                      ：查看别名
alias la = 'ls -al'    ：设置临时别名
unalias la              :   取消别名
永久设置别名个人： vim ~/.bashrc
永久设置别名全局： vim /etc/bashrc
```
**info在线帮助**
```bash
info ls 
pinfo ls
```

**man手册**
```bash
man 1 用户命令 *  man 2 系统调用 
man 3 库调用        man 4 特殊文件 
man 5 配置文件 *  man 6 游戏 
man 7 杂项            man 8 系统命令 *
man -k pass     :   模糊查询
```
<span style="background:#affad1">man -f passwd</span>： 显示passwd在那些章节有描述
![[Pasted image 20240710190538.png]]
```bash
mandb             :   刷新man手册数据库
```
**[date](https://wangchujiang.com/linux-command/c/date.html)**
date +'%Y/%m/%d   %T  %p'
![[Pasted image 20240710191244.png]]
**[type](https://wangchujiang.com/linux-command/c/type.html)**
![[Pasted image 20240710192920.png]]
别名>外部>内部
按照优先级和执行顺序，当输入一个命令时，Shell首先检查它是否是一个别名，如果是，则展开别名；然后检查是否是一个内置命令，如果是，则直接执行；最后，如果既不是别名也不是内置命令，Shell将在`PATH`环境变量指定的目录中查找该命令对应的可执行文件，即外部命令。
**which**
查找并显示给定命令的绝对路径
![[Pasted image 20240710193849.png]]
**history**
查看历史命令，默认可以存储1000条历史命令
调用之前的命令
！+历史命令首字母/命令 
！+历史命令编号

## 1. 文件管理
ls
```bash
-a -A -l -d -h -t -r -S -R
```
![[Pasted image 20240710091046.png]]
第一段: 文件类型 
```bash title:文件类型
文件类型:(7种) 
- 普通文件 file 
d 目录文件 directory 
c 字符设备文件 character 
b 块设备文件 block 
s 套接字文件 socket 
p 管道文件 pipe 
l 符号链接文件(软链接) symbolic
```
 第二段: 基本权限 
 第三段: 是否设置了ACL权限 
 第四段: 硬链接数 
 第五段: 拥有者 
 第六段: 所属组 
 第七段: 大小(字节),b4it8=1Byte 1024 1K M G
 第八段: 最后一次修改时间 
 第九段: 文件名

cd
```bash
.        当前目录
..       上级目录
cd .   刷新目录
cd -  上次工作目录
```
linux的HFS标准
Filesystem Hierarchy Standard，简称<span style="background:#affad1">FHS</span> -文件系统层次结构标准
"/" 跟目录下每个目录的作用 
<span style="background:#affad1">bin</span>        用户可执行目录(命令 root 和 普通) 
<span style="background:#affad1">sbin </span>     系统可执行目录(命令 root) 
lib         库文件目录(32位) 
lib64     库文件目录(64位)
<span style="background:#affad1">dev</span>       设备文件目录
usr        应用程序目录
var        服务器数据目录(数据 日志) 
srv        服务器数据目录 
<span style="background:#affad1">etc</span>        配置文件目录 
tmp      临时文件目录 
boot     服务器启动目录(内核和启动文件) 
media   媒介目录(u盘,cdrom) 
mnt      其他挂载点 opt 第三方应用程序目录 
proc     伪文件系统(内核参数,进程信息,硬件信息) 
sys       伪文件系统(c 配置文件目录 内核参数,进程信息,硬件信息) 
run       进程锁目录 
<span style="background:#affad1">root</span>      root管理员家目录
<span style="background:#affad1">home</span>   普通用户家目录 

pwd
绝对路径: 从/开始的路径 
相对路径: 从当前目录开始路径或非根开头的路径
cat 
```bash
-n 显示所有行的行号(包括空行)
-b 显示有效行(不包括空行）
```
head
tail
```bash
默认前/后十行
-n number  
实时监控并自定义显示行数
tail -n 20 -f filename
```
less
more
touch
```bash
touch {a,b,c}{1..3}.txt
touch /opt/file7 /tmp/file8
```
mkdir
```bash
mkdir /opt/cc.txt /etc/tt.txt
mkdir {x,y,z}{1..5}
mkdir -pv /opt/x/y/
-v:显示提示信息
```
cp
```bash
-r:递归复制
-p:保留权限
-a:=rp
```
mv
```bash
移动/改名/重命名
```
rm
```bash
-f 强制删除 
-r 递归删除目录时使用
```

file
用来探测给定文件的类型。file命令对文件的检查分为文件系统、魔法幻数检查和语言检查3个过程。

## 2. vim文本编辑器

不仅可以编辑已存在的文件，还可以创建新文件
三种模式：命令模式，输入模式，末行模式
命令模式：用于光标移动复制删除
输入模式：编辑文本
末行模式：保存退出，设置环境显示行号制表符，设置缩进等
### 2.1 命令模式：
#### (1)进入输入模式
<span style="background:#affad1">a </span>    当前字符后输入
A     当前行行尾输入
<span style="background:#affad1">i</span>      当前字符前输入
I      当前行行首输入
<span style="background:#affad1">o</span>     当前行下一行输入
O    当前行上一行输入 
s     删除当前字符后输入 
S    删除当前行后输入
#### (2)光标移动
键盘上的↑↓←→ 上下左右箭头 移动光标，<span style="background:#affad1">hjkl 左下上右</span> nh:向左移动n个字符
0               将光标定位到行首
$               将光标定位到行尾
gg             将光标定位到文章首部
shift+g/G  将光标定位到文章尾部
nG /10G    将光标定位到第10行
#### (3)复制粘贴
y                复制
y^              复制光标所在位置到行首
y$              复制光标所在位置到行尾
yw             复制一个单词
yy              复制一行
<span style="background:#affad1">nyy </span>           复制光标一下的多行
p/P            黏贴(<span style="background:#affad1">p</span>当前行下一行) (<span style="background:#affad1">P</span>当前行的上一行)
#### (4)删除撤销
d               删除
dd            剪切/删除(p P)
ndd          剪切/删除n行
d^             删除当前字符到行首(不包含当前字符) 
d$/D        删除当前字符到行尾
<span style="background:#affad1">dgg </span>         从当前行删除到首行(包含当前行) 
<span style="background:#affad1">dG   </span>           删除当前行到尾行(不包含当前行)

<span style="background:#affad1">u    </span>             撤销上一步的操作
.               重复之前的操作
ZZ            保存并退出
注意：大写字母都可以用shift+小写字母代替
### 2.2 末行模式
#### (1)常用指令
shfit + ；                   进入末行模式
:w /tmp/cc.txt           另存为
:1,3w /tmp/new.txt   将1到3行另存为
:x                                如果未修改直接退出;如已修改保存并退出
:r /etc/passwd           将/etc/passwd文件内容读取到当权文件光标处
:! command               临时输入linux命令
:e!                               重新打开当前文件
:e /root/aa.txt            打开另外一个文件
:X                               加密文件 
:X                               将密码设置为空，即可取消密码
#### (2)环境设置
:set nu/nonu                                显示行号
:set list/nolist                               显示空格或者制表符
:set tabstop=16                           文件中所有制表符都设置为16
:set softtabstop=16                     只更改设置后的制表符长度
:set autoindent/noautoindent    自动缩进
set ignorecase smartcase           搜索忽略大小写
:noh                                              取消高亮

#### (3)vim配置文件
vim ~/.vimrc                                 个人永久开启行号
set nu
vim /etc/vimrc                             所有人永久添加行号
set nu 

### 2.3 可视化模式
按 v 进入可视化模式
按 V 进入行可视化模式，选择整行
按 Ctrl+V 进入可视块模式
y：复制选中的文本到 Vim 的寄存器
d：删除选中的文本
p：粘贴之前复制的文本
u：将选中的文本转换为小写
U：将选中的文本转换为大写

### 2.4 查找替换
/关键字    从上往下搜索     n下一个 N上一个
?关键字   从下往上搜索

| 名称  | 作用    |
| --- | ----- |
| s   | 行     |
| g   | 到该行结尾 |
| %   | 全文    |
| c   | 交互式   |
末行模式做替换  /old/new/
:%s/ab/xx/gc      交互式的把文本中所有行中的ab换成xx 
3,5s/ab/xx/gc     交互式的把文本中3到5行中的ab换成xx

## 3. 用户管理
Linux系统中，用户被创建时，系统都会分配一个用户ID号，简称UID。同时也会给该用户创建一个用户组，简称GID。系统通过ID号来识别用户。

| 分类    | UID     | 作用          |
| ----- | ------- | ----------- |
| 超级管理员 | 0       | 系统管理/基本所有权限 |
| 普通账号  | 1000以上  | 系统管理/使用权限   |
| 系统账号  | 201-999 | 不能登录/用于启动应用 |

| 分类     | UID     | 作用             |
| ------ | ------- | -------------- |
| 超级管理员组 | 0       | 让其他用户加入组，使用组权限 |
| 普通账号组  | 1000以上  | 让其他用户加入组，使用组权限 |
| 系统账号组  | 201-999 | 让其他用户加入组，使用组权限 |
用户组的作用，将相同兴趣用户加入组内按统一权限管理。linux系统中，用户可以加入组，组不能再加入组。
### 3.1 用户管理
增删改查
useradd，usermod，userdel，id，passwd
#### (1)useradd 添加用户
useradd 选项 选项参数 用户名

| 选项  | 作用      |                                                                                                                                                                                                                        |
| --- | ------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| -u  | uid     | id号会按当前系统最大用户id号顺延+1                                                                                                                                                                                                   |
| -g  | gid     |                                                                                                                                                                                                                        |
| -c  | comment | 用户描述信息（非必要）                                                                                                                                                                                                            |
| -d  | home    | 指定家目录位置如果，需要使用不存在的目录                                                                                                                                                                                                   |
| -s  | shell环境 | 系统默认值/bin/bash，<span style="background:#affad1">/bin/bash</span>可登录的用户，可以和系统交互；<span style="background:#affad1">/sbin/nologin</span>非交互式shell，不能登录系统，只能加载应用；<span style="background:#affad1">/bin/false</span>,不能和系统交互 |
| -G  | 附加组     | 附加组，也称为从组，将用户加入其他组中                                                                                                                                                                                                    |
| -o  | 重复uid   | 重复使用UID时使用（非常不推荐）                                                                                                                                                                                                      |
```bash
useradd -g 4000 group2  
```
![[Pasted image 20240713102246.png]]
创建一个名为 group2 的新用户，并将这个用户的<span style="background:#affad1">主要组</span>设置为GID为4000的组。但是这个组不存在不能创建。
```bash
useradd -G root tom  
```
将tom加入到root组内
![[Pasted image 20240713102943.png]]
主要组g和附加组G区别
-g gid 主要群组，修改用户gid 
-G groups 附加组，将用户加入到哪个附加组内
```bash
useradd -g root tom 
```
![[Pasted image 20240713103546.png]]
意义为指定tom的主要群组为root，该操作影响了/etc/password中的gid部分，之后使用tom创建文件时所有者为tom，所属组为root 
```bash
useradd -G root tom 
```
![[Pasted image 20240713103645.png]]
意义为给tom指定附加组为root，也就是将tom加入了root群组中，当tom访问所属组为root的文件时可以使用root组权限
```bash
useradd -u 2005 -g 2000 -G 0 -c 'test user' -d /mnt/abc6 -s /bin/bash wwh
useradd -o -u 0 test1    # 重复使用UID时使用
```

#### (2)usermod 修改用户信息
usermod 选项 选项参数 用户名
-a 保留原始附加组，加入额外的组
usermod -G wheel newuser
![[Pasted image 20240713104600.png|550]]
usermod -aG root newuser
![[Pasted image 20240713104620.png|550]]
-u  -g  -c  -d  -s  同用户添加
修改家目录的方法
方法1：
```bash
usermod -d /tmp/jack jack 
mv /home/jack/ /tmp/
```
方法2：
```bash
usermod -m -d /home/jack jack
```
-m：这个选项表示移动用户的主目录到新的路径。如果新家目录不存在，usermod 会尝试创建它。

#### (3)userdel 删除用户
创建用户时(useradd user)会在系统中产生相关文件：
/home/user 用户家目录
/var/spool/mail/user 邮箱文件
/etc/passwd中写入用户信息
/etc/group中创建对应名称的用户组

userdel user1 只删除/etc/passwd用户信息和/etc/group中组信息，而不删除家目录及邮箱
userdel -r       删除用户同时删除家目录及邮箱信息

#### (4)用户配置文件
vim /etc/passwd
<span style="background:rgba(163, 67, 31, 0.2)">wwh</span>:<span style="background:rgba(240, 200, 0, 0.2)">x</span>:<span style="background:#affad1">1001</span>:<span style="background:#40a9ff">0</span>:<span style="background:#ff4d4f"> </span>:<span style="background:rgba(3, 135, 102, 0.2)">/home/wwh</span>:<span style="background:#9254de">/bin/bash</span>
第一段:用户名
第二段:密码占位符,现已转存/etc/shadow如果删除x，用户则可以直接免密登录
第三段:uid
第四段:gid
第五段:描述
第六段:家目录
第七段:shell 
#### (5)passwd设置用户密码
passwd 选项 用户名
passwd  直接回车  修改当前用户的密码
passwd  user          给user创建密码，只有root可以给别人设置密码，普通用                                 户只能改自己的密码
方法一:交互式
![[Pasted image 20240713154914.png]]
方式二:通过标准输入的方式
```bash
echo 123456 |  passwd --stdin root
```

#### (6)切换用户
su    用户名
su - 用户名  <span style="background:#affad1">- 会切换环境变量，推荐这个方法</span>
#### (7)验证用户是否存在
id username

### 3.2 组管理
#### (1)添加组
groupadd  test
groupadd -g 3000 test
#### (2)修改组
-g 修改组ID 
-n 修改组名
```bash
groupmod -g 3000 test
groupmod -n newtest test   # 修改test组名为newtest
```
#### (3)删除组
groupdel uplooking
#### (4)gpasswd
gpasswd命令可以将用户添加到组或将用户从组中删除(<span style="background:#affad1">附加组</span>)
gpasswd 选项 组名
-a 将用户添加到组 
-d 将用户从组中删除
gpasswd -a cl mysql  将cl加入mysql组
![[Pasted image 20240713161105.png|500]]
 gpasswd -d cl root   将cl从root组删除
 ![[Pasted image 20240713161235.png|500]]
#### (5)组配置文件
 vim /etc/group
 <span style="background:#affad1">cl</span>:<span style="background:rgba(240, 200, 0, 0.2)">x</span>:<span style="background:#d3f8b6">1002</span>:<span style="background:#40a9ff">  </span>
 第一段: 组名 
 第二段: 组密码占位符号 
 第三段: gid 
 第四段: 用户列表

### 3.3 密码管理
#### (1)密码配置文件
vim /etc/shadow
<span style="background:#affad1">user1</span>:<span style="background:rgba(240, 200, 0, 0.2)">!!</span>:<span style="background:rgba(136, 49, 204, 0.2)">19917</span>:<span style="background:#ff4d4f">0</span>:<span style="background:#40a9ff">99999</span>:<span style="background:rgba(240, 107, 5, 0.2)">7</span>:<span style="background:#affad1">  </span>:<span style="background:#fff88f">  </span>:<span style="background:rgba(173, 239, 239, 0.55)">  </span>
第一列：用户名
第二列：密码      密码字段为空没有密码；密码字段不为空 加密后的字符串，表示密码已经设置  ； !! 锁定密码状态   ； * 永久不能登录系统
第三列: 密码的最后一次修改时间；从1970年1月1日至今的天数
第四列: 密码的最小时间；密码最后一次修改后多少天内不能再重复修改
第五列: 密码的最大时间(密码有效期)，最后一次修改多久后必须变更密码
第六列: 密码过期前警告时间
第七列: 密码过期后帐号宽限时间
第八列: 帐号有效期（账号失效后，无论密码是否过期都不能使用）
第九列: 保留列
#### (2)修改密码
方法一:交互式
![[Pasted image 20240713154914.png]]
方式二:通过标准输入的方式
```bash
echo 123456 |  passwd --stdin root
```

#### (3)锁定用户
usermod -L robin：这个命令会<span style="background:#affad1">锁定</span>用户 `robin` 的账户，使其无法登录系统。
usermod -U robin：这个命令用于<span style="background:#affad1">解锁</span>用户 `robin` 的账户，如果之前该账户被锁定的话
#### (4)锁定密码
passwd -l robin：锁定用户 `robin` 的密码。这将禁用该用户的密码认证，使其无法通过密码登录系统
passwd -S robin：显示用户 `robin` 的密码状态。这个命令会输出关于用户密码的详细信息，包括密码是否已锁定、最后一次更改密码的日期、密码到期日等
passwd -u robin：解锁用户 `robin` 的密码

#### (5)配置文件
/etc/login.defs (uid,gid范围)
![[Pasted image 20240713183319.png]]
/etc/default/useradd
![[Pasted image 20240713183407.png]]
### 3.4 手动管理账号
#### (1)添加用户信息
```bash
vim /etc/passwd 
rose:x:3007:3007::/home/rose:/bin/bash
```
#### (2)添加用户组
```bash
vim /etc/group 
rose:x:3007
```
#### (3)创建用户家目录
```bash
cp -r /etc/skel/ /home/rose 
chmod 700 /home/rose                 修改文件权限 
chown -R rose:rose /home/rose   修改文件所有者和所属组
```
#### (4)创建用户邮箱
```bash
cd /var/spool/mail/ 
cp -p class1 rose class1
chown rose:mail rose                      修改用户所有者为rose/所属组为mail
```
#### (5)添加用户密码登录测试
```bash
passwd rose
su - rose
```
### 3.5 重置root密码
![[Pasted image 20240720102425.png|425]]
<span style="background:#affad1">启动界面按E</span>
找到linux16这一行，光标跳到行尾，添加<span style="background:#affad1">rd.break</span>，然后ctrl+x 启动系统
![[Pasted image 20240720102908.png]]
```bash
mount -o remount.rw /sysroot/    重新挂载根目录并添加写入权限
chroot /sysroot                               切换到根目录
touch /.autorelabel                         创建/.autorelabel文件
echo 123 | passwd --stdin root    修改密码
exit
exit                                                  重启系统,用新密码进入系统
```
## 4. 权限管理
linux系统中涉及到的安全技术方面
```bash
防火墙 (限制数据包传出和经过) 
selinux 
软件权限 
文件系统权限
```
### 4.1 查看权限
Linux中普通权限也称为文件系统权限，作用是保护文件，让有权限的用户可以访问系统文件等资源，否则不能访问，linux文件系统权限，主要设置在文件上，限制对象是<span style="background:#affad1">用户</span>。
<font color="#ff0000">注意：root用户不受文件系统权限控制</font>
<span style="background:#affad1">rwx     </span>|<span style="background:rgba(240, 200, 0, 0.2)">    rwx     </span>|<span style="background:rgba(136, 49, 204, 0.2)">    rwx </span>
<span style="background:#affad1">拥有者</span><span style="background:rgba(240, 200, 0, 0.2)">  所属组     </span><span style="background:rgba(136, 49, 204, 0.2)">其他人</span>(ugo)
权限的优先级顺序
uid=0(完全控制)-->拥有者(完全控制)-->所属组(附加组)-->其他人
<span style="background:#affad1">rwx</span> 对文件及目录的含义
file：
```bash
r  ------->read    cat head tail more
w ------->write   vim
x  ------->exec    ./后者角度路径执行
```
directory
```bash
r  ------->read     ls r-x详细信息
w ------->write    touch mkdir
x  ------->exec     cd 
```
### 4.2 修改权限
chmod  <span style="background:rgba(136, 49, 204, 0.2)">ugo</span><span style="background:#affad1">+-=</span><span style="background:rgba(240, 200, 0, 0.2)">rwx</span>  filename
chmod  <span style="background:rgba(136, 49, 204, 0.2)">7</span><span style="background:#affad1">7</span><span style="background:rgba(240, 200, 0, 0.2)">7 </span> filename
### 4.3 umask权限反掩码
umask值:<span style="background:#affad1">它表示应该屏蔽掉那些权限</span>，即可以规定linux默认目录及文件权限
#### (1)umask计算规则
目录默认权限最大777
文件默认权限最大666
超级用户umask值预设0022 
普通用户umask值预设0002
0022(第一位0代表特殊权限位，后三位代表普通权限位)
**计算方式1:**
目录或文件最大权限和umask值取反
取反规则
```bash
1 1 0 0
0 1 0 1
1 0 0 0
777=111 111 111
022=000 010 010
755=111 101 101
所以目录默认权限为755
```
**计算方式2：**
直接将数字相减的话，是会出现问题的，为什么呢？
linux系统中创建文件或目录时，默认权限会被`umask`修改。`umask`不是一个简单的减法操作，而是一种<span style="background:#affad1">权限屏蔽机制</span>。
文件默认最大权限666，假设umask=022
```bash
666=rw-rw-rw-
022=----w--w-
644=rw-r--r--
-------------------------------------------------------------
666=rw-rw-rw-
033=----wx-wx
644=rw-r--r--   # 本来就没有权限再减去一个x权限自然也还是没有
```
#### (2)管理umask值
查看umask值
[root@root /]# umask
设置umask值
[root@root /]# umask 0002
查看修改后对目录的权限
[root@root /]#umask -S
给某个用户永久设置umask
```bash
echo 'umask 0033' >> ~/.bashrc
```
给所有用户永久设置umask
```bash
echo 'umask 0033' >> /etc/bashrc
```
### 4.4 修改文件所有者所属组
chown 命令可以修改所有者和所属组，作用于超级用户，普通用户不可用 
```bash
chown [options] user [:group] file
-R 递归修改
chown -R robin:upup /tmp/dir
```
chgrp  命令只可以修改所属组，作用于普通用户，修改所属组为自己的组
```bash
chgrp upup /tmp/test.txt
```
### 4.5 特殊权限
linux权限；特殊权限；acl权限；隐藏权限

| 权限   | 对应权限数字 |
| ---- | ------ |
| SUID | 4      |
| SGID | 2      |
| SBIT | 1      |
#### (1)SUID
任何用户在运行拥有suid权限的<span style="background:#affad1">命令</span>(二进制可执行文件)时,都以该命令拥有者的身份执行
两种更改方式
chmod u+s 命令 
chmod 4755 命令
注意权限位的变化
![[Pasted image 20240724103304.png]]
![[Pasted image 20240724103339.png]]
如果本身没有执行权限
![[Pasted image 20240724103533.png]]
小s会变成大S
#### (2)SGID
如果目录设置了sgid权限,在<span style="background:#affad1">目录</span>中创建的文件/目录时都要<span style="background:#affad1">继承父目录的所属组权限</span>
chmod g+s  father
chmod 2xxx directory
![[Pasted image 20240724134434.png]]
![[Pasted image 20240724134522.png]]

#### (3)SBIT
当目录设置了SBIT权限后，只能自己删除所有者为自己的文件,其他人无权删除
chmod o+t directory
chmod 1xxx directory
### 4.6 facl文件访问控制列表
文件系统权限，主要是对一堆用户做限制，但是并不能只针对一个用户或者一个组进行单独的权限控制。
facl可以帮助我们<span style="background:#affad1">实现针对单个用户或组进行权限控制</span>
#### (1)查看是否支持acl功能
首先要确认文件系统是否支持了ACL功能，可以通过<span style="background:#affad1">Default mount options的acl字样</span>来判定是否支持，RHEL8或Centos默认都支持acl。
![[Pasted image 20240725142651.png]]
权限后面的“.” 或者没有“.”，均表示未设置acl权限
setfacl -m u:lisa:r /opt/test.txt
![[Pasted image 20240725142832.png]]
后面是加号代表设置了acl权限
#### (2)acl管理
setfacl用于管理acl权限 
getfacl用于查看acl权限
<span style="background:#affad1">setfacl</span> <span style="background:rgba(240, 200, 0, 0.2)">-m</span> <span style="background:#affad1">u</span><span style="background:#fdbfff">:lisa</span><span style="background:#40a9ff">:r</span> <span style="background:#d4b106">file</span>
第一项：命令 
第二项：添加一个ACL 
第三项：针对对象是单一用户 
第四项：用户名称 
第五项：权限 
第六项：文件名
<span style="background:#ff4d4f">setfacl：</span> 
命令选项： 
-m: 设置acl 
-x：删除指定的acl 
-b：删除所有acl 
-R：递归 
```bash
setfacl -m u:lisa:r /opt/test.txt
setfacl -x u:tom file 
setfacl -b file
```
<span style="background:#ff4d4f">getfacl</span>：
-R：递归
getfacl  /opt/test.txt
![[Pasted image 20240725153443.png]]
<span style="background:#ff4d4f">ACL权限设置的对象 </span>:
u user 所有者 
g group 所属组 
o 其他人
m mask值
#### (3)mask值的特点
修改mask值
setfacl -m m:staff file
![[Pasted image 20240725154914.png|500]]
![[Pasted image 20240725155001.png|500]]
![[Pasted image 20240725155109.png|500]]
![[Pasted image 20240725155214.png|500]]

1.对文件设置了acl权限后,mask值产生,或刷新旧mask值。除非手动设置，否则mask值会首先满足最大facl中的权限。 
2.我们可以通过调整mask值来限制acl权限的大小范围，也会影响基本组权限(有效权限范围)。
3.mask值权限和facl权限重叠部分，为有效权限
4.mask值权限不仅会显示在getfacl查询表中，也会同步到文件<span style="background:#affad1">所属组权限位置</span>。 
5.文件一旦设置了acl权限，需要使用getfacl来查看文件<span style="background:#affad1">真实权限</span>及acl权限。 
6.当删除单个acl之后，使用ll查看，发现在改文件或者目录不存在任何acl的情况下，仍然存在一个加号，显示存在acl设置，<span style="background:#affad1">使用getfacl查看发现有mask值残留</span>，使用命令setfacl -x m file ，删除之后再ll查看发现没有加号。 
<span style="background:#b1ffff">7.文件设置acl权限后，chmod命令修改的是mask值，不是文件基本组权限。</span>
#### (4)修改文件原<span style="background:#affad1">用户组</span>权限
setfacl -m g::rwx test2.txt
#### (5)acl备份及还原
```bash
setfacl -Rm u:harry:rwx /opt/dir
getfacl -R /opt/dir > /acl.bak                 # 将权限备份到/acl.bak文件中
setfacl -Rb /opt/dir                                # 清除，模拟丢失 
getfacl -R /opt/dir                                  # 此时没有权限 
setfacl -R --set-file=/acl.bak /opt/dir   # 恢复 
getfacl -R /opt/dir                                  # 再查看权限已经恢复
```

### 4.7 隐藏权限
lsattr 文件名              # 查看隐藏权限
chattr [选项] 文件名  # 修改隐藏权限
选项： 
-i：不能写入和删除 
-a：可以追加数据，不能删文件
![[Pasted image 20240725162927.png]]
chattr +a 1.txt  
lsattr 1.txt
![[Pasted image 20240725163002.png]]


## 5.软件包管理

按安装方式划分
rpm包管理
源码包管理
linux本地的rmp包存放位置
![[Pasted image 20240725165118.png]]
软件包结构
<span style="background:#affad1">nmap-6.40-7</span>.<span style="background:rgba(240, 200, 0, 0.2)">el7</span>.<span style="background:#affad1">x86_64</span>.<span style="background:#fdbfff">rpm </span>
<span style="background:#affad1">包名-版本</span>.<span style="background:rgba(240, 200, 0, 0.2)">系统版本</span>.<span style="background:#affad1">平台</span>.rpm
安装软件
rpm 选项 软件包全名 
rpm -ivh /mnt/Packages/telnet-0.17-64.el7.x86_64.rpm 
-i  安装 install 
-v 显示过程 view 
-h 显示%
RPM 包默认安装路径
/etc/                          配置文件安装目录 
/usr/bin/                   <span style="background:#affad1">可执行的命令安装目录 </span>
/usr/lib/                     程序所使用的函数库保存位置 
/usr/share/doc/        基本的软件使用手册保存位置 
/usr/share/man/       帮助文件保存位置

软件包的查询
查询是否安装
rpm -q nmap  # 不加.rpm后缀
查询包的信息
rpm -qi nmap















# Openstack
云操作系统
提供了一个构建云服务的框架
## 1.云的简介
大量的虚拟化的资源池
云的核心底层技术：虚拟化
### 1.1 优势
按需自助的服务
广泛的网络接入
弹性伸缩
资源池化
### 1.2 根据云平台提供的服务不同
Iaas
	基础设施(服务器、存储、网络设备)即服务
	Infrastracture As a Service
Paas
	平台即服务
	Platform As A service
Saas
	软件即服务
	Software As A Service
![[Pasted image 20240711220044.png]]
### 1.3 根据提供服务范围不同
- 公有云
- 私有云
- 混合云
## 2.openstack核心组件
云操作系统，构建云平台提供了一个框架
现在外面一些做云的厂商，他们自己的私有云产品都是在openstack的基础上做了大量的二次开发，然后形成了自己的产品
<span style="background:#affad1">要对openstack里面的一些核心组件要有一个整体的认知</span>
openstack是一堆软件的组合，统称为openstack
<font color="#d99694">openstack在云平台中起什么作用？</font>

openstack版本 
命名是按字母命名的，ceph这个软件的命名就是随openstack来的，也是按英文字母命名

核心组件
nova glance neutron cinder switft keystone heat lronic dashboard
![[Excalidraw/Drawing 2024-07-10 19.49.48.excalidraw.md#^group=FRBwRiMKyPVnAMyw6VAB5|700]]
云上这些所有的组件都是给<span style="background:#affad1">云服务器(VM)</span>服务的
### 2.1 Nova组件
<span style="background:#affad1">提供计算compute服务</span>
负责云服务器(计算资源)的<span style="background:#affad1">整个生命周期管理</span>(创建，查看，更新，删除)
我们在任何一个云平台上面，之所以能购买云服务器，配置好参数以后这个云服务器就能够被真正的创建出来，然后这个云服务器创建相关的所有操作都是由Nova这个组件负责完成的

**Nova的核心组件**
组件内部有设计了很多的子部件
![[Pasted image 20240711204737.png|525]]

| 名称               | 作用                             |
| ---------------- | ------------------------------ |
| nova-api         | 接收关于云服务器的操作请求                  |
| nova-scheduler   | 调度操作请求                         |
| nova-compute     | 调用计算节点的Hypervisor真正实现云服务器创建、删除 |
| nova-conductor   | 与数据库交互，处理云服务器的元数据信息            |
| nova-consoleauth | 提供认证，认证登录云服务器时，用户名、密码、密钥       |
| nova-novncproxy  | 提供VNC的方式登录服务器                  |

nova-api 
主要负责接收有关于云服务器的操作请求，将来你要拿openstack搭建云平台，无论是通过UI还是通过命令行，你要搭建云服务器或者删除云服务，总要有人来接受你的请求，这个就是nova-api做的事情
nova-scheduler是用来做调度的
按照官方给我们的建议自己要去搭建openstack的话，至少要两个机器
![[Pasted image 20240711205452.png|450]]
一个控制节点，一个计算节点
控制节点需要装一些openstack运行必备的一些基础组件（时间同步，mysql，消息队列，memory catch等）nova在设计的时候内部会涉及很多个子进程，当创建云服务器的时候，要传递很多数据，为了实现同一个组件内部多个不同的子进程之间的高效通信，需要消息队列的支持，memory catch跟redis类似，是用来做缓存的，存一些用户的令牌，用户的一些认证信息
计算节点，云上边所创建出来的那个云服务器，本质上就是一个kvm虚机，计算节点就是将来跑虚机的结点。所谓的云服务器，就是跑在计算节点上的一个虚拟机，要是做一个稍微有点规模的云平台，计算节点不可能只是一个（成百上千）
我们就拿创建来说，第一个进程叫做nova-api，他收到一个请求，这个api肯定能从这个请求里面看到我要建一个云服务器的名字，我要什么样子的规格，我的镜像是什么样的，这些请求里面应该包含这些数据，这些数据叫做云服务器的元数据，接收到这些请求之后，再通过keystone验证这个请求是合法的后，这个api会联系DB，把云服务器的元数据存储在这个数据库里面
nova-scheduler这个调度，是调度什么的呢？创建云服务器的时候，这个机器应该建在那个计算节点上？假设我们这个规模里面有几十甚至几百个计算几点，这么多计算节点存在，是不是应该有一个所谓的调度，帮我们选择一个合适的节点，来真正创建这个云服务器，这就是调度。
调度就是负责选择一个合适的计算节点，再去真正的去创建这个云服务器

nova-compute是跑在计算节点上的一个进程，我们之前说，nova节点是负责创建删除云服务器的，但是这个创建删除的操作，并不是nova做的，而是hypervisor
nova这个组件他本身并没有创建删除虚拟机的功能，其实是这么一回事
api接受到一个请求，然后这个请求通过scheduler调度帮我们去选择一个合适的计算节点创建虚拟机，那这个云服务器是怎么建出来的？我们假设在第一个节点上面创建虚拟机。这个调度，把这个请求调度到第一个节点，这个请求是由一个叫做nova computer的节点接收的
他接收到有人给我发了一个请求，让我去建一个虚拟机，但是nova本身并没有真正创建删除的功能，真正的创建删除是靠我们装在这计算机点上的hypervisor实现的（默认使用的是kvm）

nova-api接收到一个创建请求，然后又这个调度帮他选着一个合适的计算节点，然后这个计算节点选择了之后，再把这个请求转交到这个计算节点身上的compute，nova-computer,真正拿到这个要创建或者删除的请求之后，他会去调用我们在计算节点上真正装的这个hypervisor来真正的实现创建，真正的实现删除
现在所有的hypervisor都是硬件复制的虚拟化，将来做基础环境准备的时候，所有的计算节点必须开启虚拟化功能

nova-compute接收到要创建这个服务器的请求，他确实要调用hypervisor真正的把这个虚机建出来，但他在见这个虚拟机的时候，他是不是应该需要知道这个虚拟机的名字叫什么？应该分配多大的硬盘，应该介入到那个网络当中去，他是不是应该知道这些信息呀，但是他是怎么 知道这些信息的呢？这就涉及到另一个组件叫做nova-conductor

nova-conductor,真正调用hypervisor之前nova-compute 是不是要去数据库里面拿到这个要被创建的服务器的元数据信息，拿到之后，再去按照这个元数据，再去调用hypervisor把服务器按需创建出来，谁来帮nova-compute 去数据库里面那这个信息呢？就是中间的nova-conductor
这就是核心的四个进程，这四个进程一个不成功，虚拟机就创建不出来



Nova只是单纯的提供建服务器的功能，但是这个服务器真正能创建成功，还需要其他组件的配合，比如说创建虚拟机的时候需要选择镜像，这个镜像谁提供的？就是下面的glance组件

### 2.2 Glance组件
提供镜像image服务
负责镜像的上传，查看，删除
结合虚拟化来说，这里所说的镜像，其实就是一个.qcow2格式的磁盘文件，或者是.iso格式的系统镜像文件
两个子进程
glance-api
接收关于镜像的api请求的调用，包括镜像的查看更新存储
![[Pasted image 20240712183404.png]]
glance在存储镜像的时候支持多种存储方式，S3亚马逊对外提供的一个对象存储服务
glance-registry 就是用来和后台的数据库交互，来存储一些和镜像相关的数据(镜像的元数据信息)。open stack里面的每个组件运行的时候都会产生数据，所以都需要关联后端数据库存储


### 2.3  Cinder组件
提供卷volume服务
负责为云服务器提供卷服务(云硬盘/虚拟磁盘)/虚拟磁盘的整个生命周期的管理

如果你要子啊你的云平台里面构建这个卷服务的话，建议在增加一个节点block storage 块存储节点
![[Pasted image 20240712200416.png]]
无论是你服务器本地插了很多硬盘也好，还是说你的存储节点后面关联了很多存储设备，你既然作为整个云平台对外提供块存储服务，那其实，将来就是你得把你提供的这个零散个磁盘空间给集中到一起，然后在往外边分，也就说先虚拟出一个大的存储池，然后在根据用户的需求把磁盘空间划分出去。
这些零散的空间通过什么技术能够集中到一起去？
通过lvm集中到一起去，云磁盘底层里面涉及到的一个核心技术就是当初学的逻辑卷，把所有的磁盘形成一个卷组，通过这个卷组就想到于把所有的空间集中到了一起，云平台搭好以后，我们的这个终端用户购买硬盘，无非就是在这个卷组里面给我划分一个一个的卷

按照我们的open stack网络拓扑来说，其中有一类是计算节点，计算节点就是用来跑虚拟机的，现在有一类存储节点，他底层可以通过lvm这样的底层技术，把所有的空间集中起来，然后再往外分配，假设云平台搭设完成以后，有人购买一个5GB的硬盘，其实就是底层的lvm检出了一个5G的卷，那着里面有什么问题呢？这个卷肯定是建立在存储节点上的，而你买的云服务器是跑在计算节点上的，那从最终使用的角度来讲，我肯定是要把这个硬盘，加到某一个云服务上面去，这里面涉及到的问题就是，你的云服务器在计算节点上，你的5G的硬盘在存储节点上，怎么通过什么东西把这个5G的磁盘关联到计算节点上？
能往外直接共享块设备的，按照我们之前说的存储就是san存储，按照协议来说就是iscsi协议，将来用户在我这购买了虚拟磁盘之后，为了这个磁盘能够关联到某一个云服务器上面去用，采用的就是共享的方式，走的就是iscsi协议和scsi协议
什么时候走iscsi呢？就看你中间所有设备的网络连接，如果走的是IP网络走的就是iscsi，如果走的是光纤网络走的就是scsi了。

涉及的技术：LVM、iSCSI/SCSI共享块设备
cinder组件的核心进程
![[Pasted image 20240712202744.png|450]]

cinder-api，负责接收请求、路由请求到cinder-volume上
cinder-scheduler，负责选择合适的存储节点
cinder-volume，负责调用底层的存储接口/驱动真正创建、销毁虚拟磁盘
cinder-backup，负责提供磁盘备份服务，配合swift对象存储使用






### 2.4 neutron组件
提供网络network服务
负责为云服务器提供网络连接
比如：vpc;sg安全组;nat;vpn;负载均衡器
早年的iaas主要是提供基础设施服务的，基础设置就三样，计算；网络；存储
单纯是iaas平台的话 nova glance cinder neutron 是最核心的组件
整个所有的openstack里面所有的组件设计最复杂的就是neutron
核心进程就一个
neutron-server
接收用于关于到我们这个云平台上面网络创建呀，查看完了网络接入这些请求，接收到请求之后，neutron-server还会把这些请求转发到合适的网络插件上面
单说neutron，是提供网络连接的，但是从整个open stack的设计的角度来讲，这个云平台上面所有的网络功能都是通过各个插件来的
![[Pasted image 20240712190011.png]]
网络的元数据信息也是要存到后边的数据库的
每个插件和neutron-server高校交互的时候都通过消息队列

所有的云平台后端核心的技术就是虚拟化技术

网络虚拟化技术
vpc怎么实现的？
最基本的网络功能是靠Open vSwitch 实现的，这就是网络上的一个虚拟化技术，对于虚拟话网络里面所说的那个虚拟网络就相当于模拟生活中的交换机的功能，它主要用到的技术就是open vswitch 
Linux bridging 桥接技术

vxlan 
在云平台上面，你创建一个vpc你就要指定一个网段，这个网段我可以指192；你也可以指定为192，对于华为云的后端而言，他怎么支持这么多人在他身上建出来这么多的虚拟网络呢？单纯从数量和网络隔离上面来说，他用的是什么技术呢？用的技术就是vxlan
vlan是用在交换机上的技术，vlan能够实现网络隔离的功能(划分广播域)要给，一个交换机上可以创建4096个，交换机在碰到广播类型的数据的时候他默认会泛洪
单纯的虚拟化环境里面那一个虚拟网络就相当于一个vlan，从云的角度来说，我们也可以理解为一个vpc就是一个vlan。
就刚才我们说，我们建了多个vpc他是怎么帮我们实现网络隔离？思想是vlan的这个思想，但是用技术不是vlan这个技术，应为你在建vlan的时候一个交换机上面最多只能建立4096个vlan，如果放在现实中的环境肯定是够的，但是放在云上肯定是不够的，所以自动虚拟化技术出现之后vlan技术也发生了一个改革，产生了vxlan，vxlan相当于vlan核心有两个好处，第一个很大程度上扩展了这个数量（不受数量的限制）
第二个好处就是，在云上或者从物理环境上来说，给我们做设备迁移带来了很大的便利，
平常的迁移IP地址会变，IP地址变了会影响业务的，牵一发而动全身，而vxlan呢，在原始的数据包上的最外层又增加了一个二层封装，然后就可以实现，将来主机也好，服务器也好，他在跨网段迁移的时候可以保存IP不变。
数据封装要封装个样东西，源端口目的端口，源IP目的IP，端mac目的mac，这是传统的数据封装，如果这个数据是跨网段传输的，它必然需要经过路由，传统的路由是根据目的ip来转发数据的，所以vxlan改变了传统的根据IP的方式转发数据的方式，他子啊传统的数据包上又多加了一层新的mac地址的封装，有了这个新的mac地址的封装，哪怕你这个地址和以前是不同网段的，我不按照IP的方式走，我按照mac地址的方式，这样按网络上来说，我就可以实现一个2层的跨传，哪怕这两个设备不在统一个网段里面，我一样可以通过mac地址的方式帮你把数据转出去，但是中间要经过一个vxlan的隧道，而且你的网络设备必须要买支持vxlan功能的网络设备(主流)


sdn
软件定义网络
主要实现的是网络设备的控制平面和数据平面的隔离，说白一点就是实现网络设备的自动化
在云上面我建了一些vpc，很多人都会建很多vpc，虽然对于我们终端用户来说我们不同的用户之间的资源是隔离的，但是对于整个华为云的后台而言，我们建的所有的东西都是表现在一个云平台里面的，他是一个整体，假设华为内部人员要对云上的vpc进行策略上的更新，但是一个个改效率太低了，这个时候就可以用到sdn，通过sdn就可以实现集中的向所有的网络设备下发统一的配置
sdn又分两类
硬sdn 专业的硬件，单独的设备(sdn控制器)
软sdn  软件/平台，来对网络做集中的控制

### 2.5 swift组件
提供对象存储服务object storage /对象存储
负责存储磁盘镜像文件，磁盘备份文件

配合glance用来存储镜像或cinder卷的备份
单说glance他是负责云平台后端的这个镜像的上传下载，比如说我们将来这个云平台构建好之后，我们要大量的往云平台上面传一些镜像，然后glance支持很多种方式来存储这个镜像。
比如你要是最小化部署的话，你可以在服务器下面建一个文件夹然后配置glance使用这个文件夹来存储镜像，按真正的云平台构建来说，云平台的后端肯定有专业的存储设备，然后glance也支持去关联后端的设备来存储这些对象
，或者就是在云平台里面装一个swift，这就相当于云平台里面有了对象存储服务，将来也可以配置我的glance去关联swift这个对象存储，在这个对象存储里面存储上传的镜像文件

还有一方面提供云硬盘备份服务，云硬盘备份服务按照单纯的虚拟化来说就是，对硬盘建快照

### 2.6 cellometer组件
提供计量服务
计量服务不单单指计费，比如说这个云服务器是什么时候买的，用了多长时间什么时候删的，然后从费用来说，应该扣多少钱。你买的公网IP什么时候买的，用了多少带宽多少流量怎么计费
### 2.7 keystone组件
提供认证identity服务，最最核心的组件
负责用户的认证；鉴权
这个认证包含两个方面
(1)用户的认证
(2)我们需要有个整体意识，虽然我们是openstack里面有这么多组件，各干各的事，但从整个云平台的实现角度来讲，这些组件和组件之间是有关联的，而且这些组件是多个团队开发的，这些组件在相互传递数据，相互沟通的时候，他们需要找keystone做认证
### 2.8 dashboard组件
提供web UI 界面，web UI 可有可无，因为openstack支持命令行方式

### 2.9 heat组件
提供编排服务
编排服务就是说，一创建云服务来说，常规的方式就是在web界面上面选择参数，所谓的编排呢，就是创建资源的另一种方式，后面在K8S里面创建资源都是以所谓的编排的方式


# Docker
<span style="background:#affad1">容器和传统虚拟机的区别</span>
## 一.容器介绍
轻量级虚拟化技术
创建速度特别快；秒级
创建了 保证某业务正常运行的必备的应用程序/指令
![[Pasted image 20240715093641.png]]
为什么创建速度快？
1.容器是没有自己的虚拟硬件的，没有独立的内核
2.共享物理机内核，所以容器的IO速度要比传统的虚拟机快
 容器里面只有能维持那个软件正常运行必要的一些指令
假设有一个mysql镜像，创建出了mysql容器，从容器创建好的那一刻开始，mysql软件都是装好的，mysql相关的指令也都存在

![[Pasted image 20240715190311.png]]
<span style="background:#affad1">传统的虚拟机</span>要上网，从app产生一份数据，交给虚拟机的虚拟内核，如果磁盘或者网卡装了半虚拟化驱动的话，这个数据会直接绕过虚拟硬件到hypervisor虚拟机管理软件，再到真实物理机的内核，然后再通过真实物理机的网卡出去
<span style="background:#affad1">容器</span>他并不是一个完整的系统，它上面只是有一堆应用程序，所以他们是共享物理机内核，这就意味着容器将来想借助你物理网卡上网的话，容器产生的数据直接就能到物理机内核然后通过物理网卡转出去

有容器之前和没容器的时候有什么区别？
(1)快速的构建业务，根据公司业务自定义镜像
(2)从业务迁移的角度叫，提供了很大的便利

1.容器的优势
快速部署业务环境；平台
便于业务迁移，避免兼容性问题

容器的三要素
容器，镜像，仓库
![[Pasted image 20240715102954.png]]
容器相关的核心技术
容器之间是怎么实现资源隔离的？
传统的虚拟机通过虚拟内核就可以实现资源隔离
容器资源隔离
<span style="background:#affad1">namespace 命名空间</span>
实现资源隔离(文件目录，用户，端口，进程)
创建容器的时候会自动生成一个命名空间，然后吧容器放到这个空间里面实现资源隔离
<span style="background:#affad1">cgroup 控制组</span>
实现容器的资源限制(cpu,内存)
实际的生产环境里面，每个机器必须做资源限制

容器的管理工具/软件
docker
docker-ce;docker-ee
podman 跟docker一模一样
containerd

## 二.安装docker
### 1.配置docker软件仓库
wget -O /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo
cat /etc/yum.repos.d/docker-ce.repo
```bash
[docker-ce]
name=docker-ce
baseurl=https://mirrors.aliyun.com/docker- ce/linux/centos/7.9/x86_64/stable/
enabled=1
gpgcheck=0
```
### 2.安装docker, 启动docker服务
yum install -y docker-ce
rpm -q docker-ce
![[Pasted image 20240715111547.png]]
systemctl enable --now docker
### 3.配置国内docker镜像仓库
cat /etc/docker/daemon.json
![[Pasted image 20240715111655.png|500]]
![[Pasted image 20240715111723.png]]
### 4.主机网络变化
ifconfig
![[Pasted image 20240715111904.png]]
cat /proc/sys/net/ipv4/ip_forward
![[Pasted image 20240715111938.png]]
iptables -t nat -nL
![[Pasted image 20240715112538.png]]


## 三.容器管理的常用操作
创建容器
我是用什么镜像创建容器，设置容器执行什么命令
docker run [选项] 镜像名称 [命令]
-t -i 提供一个终端命令提示符
-d 后台运行
<span style="background:#affad1">任何一个容器的运行必须依赖于一个持续运行的进程存在</span>
docker run -tid centos:7 /bin/bash
### 1.查看容器
docker ps -a
### 2.查看容器的详情
docker inspect id
### 3.查看容器的日志
docker logs  id
### 4.连接登录容器
docker exec -ti id bash
### 5.删除容器
docker rm id 
docker rm -f id 强制删除
### 6.启动/停止/重启容器
docker [start|stop|restart] 容器ID/名称
### 7.杀死容器
docker kill id
### 8.导出容器
docker export -o suiyi.tar id 
### 9.导入容器
docker import suiyi.tar
![[Pasted image 20240715202714.png]]
导入之后还是一个镜像，然后还是要根据这个镜像重新创建容器
![[Pasted image 20240715202731.png]]
修改名字跟tag
docker tag 555308ffc1d7 suiyi:1003
docker run -tid suiyi:1003 /bin/bash
<span style="background:#affad1">docker run -tid --name xxx  image_id /bin/bash</span>
![[Pasted image 20240715202855.png]]

## 四.容器常用选项
### 1.后台运行
-d 
### 2.指定容器名字 主机名字
--name=test1 --hostname=test1 
```bash
docker run -tid --name=test1 --hostname=test1 centos:7 
docker exec -ti test1 bash
```
### 3.设置开启自启
--restart=always 
```bash
docker run -tid --name=test2 --hostname=test2 --restart=always centos:7 
```
### 4.发布容器内容
<span style="background:#affad1">-p 物理机端口:容器端口</span>
```bash
docker run -tid --name=test4 --hostname=test4  nginx:1.18 
```
现在我们机器里的这个80端口谁可以访问？
跑docker的这个物理机可以访问，其他docke容器也可以访问
如果想外界可以访问，第一种方式就是通过nat转换(dnat)
docker思想上是这样的，但是操作不是这样

容器一点创建好以后，里面的东西就已经固定了没法改变
```bash
docker run -tid --name=test4 --hostname=test4 -p 80:80 nginx:1.18 
```
![[Pasted image 20240715203734.png]]
![[Pasted image 20240715210340.png]]
![[Pasted image 20240715210553.png]]
-P 随机发布端口
docker run -tid --name=test6 --hostname=test6 -P nginx:1.18 
从三万多开始

### 5.传递环境变量
https://hub.docker.com/
-e 变量名称=值
docker run -tid --name=test7 --hostname=test7 
-e NAME=martin centos:7 
![[Pasted image 20240715210950.png]]
 docker run -tid mysql:5.7 
![[Pasted image 20240715211226.png]]
![[Pasted image 20240715211247.png]]
容器建出来就是挂的
查看日志
docker logs  4a3a
![[Pasted image 20240715211408.png]]
后面必须添加参数传递环境变量

```bash
docker run -tid --name=test8 --hostname=test8 
-e MYSQL_ROOT_PASSWORD=redhat mysql:5.7 

docker run -tid --name=test9 --hostname=test9 \
> -e MYSQL_ROOT_PASSWORD=redhat \
> -e MYSQL_DATABASE=it \
> -e MYSQL_USER=admin \
> -e MYSQL_PASSWORD=redhat \
> mysql:5.7
```
![[Pasted image 20240715211653.png]]

### 6.容器持久化保存
<span style="background:#affad1">-v 物理机目录:容器目录</span>
docker run -tid --name=test1 --hostname=test1 -v /opt/test1:/test1 centos:7 
docker exec -ti test1 bash
![[Pasted image 20240715211858.png]]
查看镜像对应的持久化目录， 使用docker image inspect查看详细信息找Volumes关键字
docker image inspect 8cf625070931
![[Pasted image 20240715212202.png]]\
```bash
docker run -tid --name=test2 --hostname=test2 \
> -e MYSQL_ROOT_PASSWORD=redhat \
> -v /opt/test2/:/var/lib/mysql \
> mysql:5.7 
```
多个容器挂载同一个目录
docker run -tid --name=test3 --hostname=test3 -v /opt/test1/:/test3 centos:7 
 docker run -tid --name=test4 --hostname=test4 -v /opt/test1/:/test4 centos:7 
docker exec -ti test30 bash
![[Pasted image 20240715212630.png]]
![[Pasted image 20240715212639.png]]
![[Pasted image 20240715212649.png]]



### 7.容器中应用的配置文件
-v
把配置文件在物理机上准备好

![[Pasted image 20240716091034.png]]
![[Pasted image 20240716091115.png]]
### 8.定义容器的通信别名

--link=容器名:别名
容器不允许使用IP通信
### 9.容器的资源限制
--cpus
--memory

### 10.小练习
前期规划：
#### (1)mysql 主从
主 配置文件
/opt/mysql_master
[mysqld]
server_id=1
log_bin=master
gtid_mode=ON 
enforce_gtid_consistency=true 
从配置文件
/opt/mysql_slave
[mysqld]
server_id=2
log_bin=master
gtid_mode=ON 
enforce_gtid_consistency=true 

创建主虚拟机
```bash
docker run -tid --name=mysql_master --hostname=mysql_master -v /opt/mysql_master:/etc/my.cnf \
--cpus=1 --memory=400M \
-e MYSQL_ROOT_PASSWORD=redhat  mysql:5.7
```
创建从虚拟机
```bash
docker run -tid --name=mysql_slave --hostname=mysql_slave \
--link=mysql_master:mysql_master \
-v /opt/mysql_slave:/etc/my.cnf \
--cpus=1 --memory=400M \
-e MYSQL_ROOT_PASSWORD=redhat mysql:5.7
```

用户的创建再mysql内部创建
docker exec -ti mysql_master bash
mysql> create user 'repluser'@'%' identified by '[WWW.1.com](http://WWW.1.com)';
mysql> grant replication slave ON _._ TO 'repluser'@'%';
mysql> FLUSH PRIVILEGES;

docker exec -ti mysql_slave bash
mysql> CHANGE MASTER TO 
-> MASTER_HOST="mysql_master",
-> MASTER_USER="repluser", 
-> MASTER_PASSWORD="[WWW.1.com](http://WWW.1.com)", 
-> MASTER_AUTO_POSITION=1; 
mysql> START SLAVE;
mysql> SHOW SLAVE STATUS\G;
![[Pasted image 20240716105941.png]]
主从复制完毕


#### (2)创建业务容器
wordpress

首先主库创建wordpress连接用户
create database wordpress charset utf8;
create user 'wpuser'@'%' identified by 'WWW.1.com';
grant all on wordpress.* to 'wpuser'@'%';
flush privileges;

```bash
docker run -tid --name=wordpress1 --hostname=wordpress1 \
--link=mysql_master:mysql_master \
--cpus=1 --memory=1024M \
-e  WORDPRESS_DB_HOST=mysql_master \
-e  WORDPRESS_DB_USER=wpuser \
-e WORDPRESS_DB_PASSWORD=WWW.1.com \
-e WORDPRESS_DB_NAME=wordpress  \
-p 80:80 wordpress

```

```bash
docker run -tid --name=wordpress2 --hostname=wordpress2 \
--link=mysql_master:mysql_master \
--cpus=1 --memory=1024M \
-e  WORDPRESS_DB_HOST=mysql_master \
-e  WORDPRESS_DB_USER=wpuser \
-e WORDPRESS_DB_PASSWORD=WWW.1.com \
-e WORDPRESS_DB_NAME=wordpress  \
-p 81:80 wordpress
```

![[Pasted image 20240716112929.png]]
测试成功

#### (3)负载均衡
haproxy镜像制定的用户是普通用户，普通用户不能使用1024 一下的端口
```bash
docker run -tid --name=haproxy --hostname=haproxy \
--link=wordpress1:wordpress1 \
--link=wordpress2:wordpress2 \
--cpus=1 --memory=1024M \
-v /opt/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg  \
-p 8000:8000 haproxy
```
准备haproxy配置文件
vim /etc/haproxy/haproxy.cfg 
![[Pasted image 20240716113954.png]]


```bash
frontend web_service
   bind 0.0.0.0:8000
   mode http
   option forwardfor	
   use_backend blog			
backend blog
   balance roundrobin
   mode http
   server wordpress1 wordpress1:80 check
   server wordpress2 wordpress2:80 check
```

容器和物理器之间相互cp数据
docker cp wordpres1:/wordpress.sql   ./


## 五.堡垒机/跳板机
![[Pasted image 20240716160037.png]]
### 1.jumpserver安装使用
#### (1)生成jumpserver需要的密钥
```bash
cat /dev/urandom | tr -dc a-zA-Z0-9 | head -c 50
B5Df26hgzQffkqPYCodUmhDhrcIl7OlJuYxBBnzpbnKQnjoc5u
cat /dev/urandom | tr -dc a-zA-Z0-9 | head -c 20
yTn6VcSyW7VBs2kGvoOR
```
#### (2)创建容器运行jumpserver
```bash
docker run -tid --name=jumpserver --hostname=jumpserver \
 --restart=always \
 -p 83:80 -p 2222:2222 \
 -e SECRET_KEY= B5Df26hgzQffkqPYCodUmhDhrcIl7OlJuYxBBnzpbnKQnjoc5u \
 -e BOOTSTRAP_TOKEN=yTn6VcSyW7VBs2kGvoOR \
 -v /opt/jumpserver/data:/opt/jumpserver/data \
 -v /opt/jumpserver/mysql:/var/lib/mysql \
 jumpserver/jms_all:v2.8.4 
```
![[Pasted image 20240716192221.png]]
#### (3)客户端登录
ssh admin@192.168.140.21:2222 
![[Pasted image 20240716192152.png]]

2.jumpserver的使用

创建用户组
创建用户
![[Pasted image 20240717091808.png]]
添加资产
创建系统用户----创建管理用户-----添加资产
创建系统用户
跳板机登录后端资产使用的用户，该用户应该在资产服务器上存在
![[Pasted image 20240717092658.png]]
创建管理用户
获取资产的配置信息
![[Pasted image 20240717092940.png]]
添加资产
![[Pasted image 20240717093511.png]]
用户授权/资产授权
suiyi,huqian这两个账号授权
![[Pasted image 20240717094026.png]]
测试连接
![[Pasted image 20240717094212.png]]

## 六.docker容器网络管理
### 1.docker网络的工作模式
docker network ls 
![[Pasted image 20240717112357.png]]
四种bridge host  container none
#### (1)bridge模式(nat)
实际就是NAT模式  
SNAT：网关、路由转发、SNAT规则  
DNAT：-p, -P；注意端口冲突
#### (2)host模式
容器会和物理机共享一个网络命名空间
容器会和物理机共享一个IP
docker run -tid --net=host centos:7
登录上之后跟物理机一个主机名
但是并不是同一个机器
容易出现端口冲突

#### (3)container模式
新建的容器会和已有的容器(bridge)共享同一块网卡
docker run -tid --name=test5 --net=container:test4 centos:7
减少容器之间的网络消耗
#### (4)none模式
容器没有自己的网络命名空间

## 七.flannel+etcd网络
跨物理机之间的容器相互通信
一种方法:host网络模式
第二种方法：
假设在物理机之间，在物理机的容器环境之间假设存在着一条特殊的路线或者说存在一条特殊的网线
![[Pasted image 20240717213818.png]]
把这两个物理机上的容器环境给串起来
但是你需要去改容器网络的分配方式
### 1.flannel工作原理介绍
解决跨物理界容器间通信的问题：
1.改变容器的ip分配方式
2.特殊的线路来连接容器网络

要通过flannel这个软件解决这两个物理机上面的容器通信问题，这两个物理机上面我们都需要安装这个软件

借助这个flannel软件可以在所有的物理机的基础上，形成一个大的虚拟网络
这个虚拟网络在形成的时候，需要我们通过配置人为的指定一下，你这个虚拟网络想用那个网站的地址，这个网段随便指，但是不要和现实网络里面 真实存在的网络冲突
规划16位掩码的虚拟网络

在物理机上面安装启动这个flannel软件之后，他会在你的物理机上面，有会形成一块虚拟网卡，默认叫做flannel0
这就相当于我左侧的这个物理机通过这个flannel0的这个虚拟网卡，就自动接入到了上层的虚拟网络上面
一旦接入成功之后，上面的这个10.88.0.0/16网段会自动给flannel自动分配一直子网，分什么不一定，但是肯定可以保证给每个物理机分配的子网是不冲突的
![[Pasted image 20240717214947.png]]
假设分了一个10.88.2.0/24的这么一个子网
接入到虚拟网络上，这个虚拟网络给物理机分配了一个子网这是第一
第二，我们在物理机上面安装好flannel之后，我们还需要让flannel接管物理机上面的docker0网卡
一旦接管成功之后，就相当于我们装的flannel把我们机器上docker默认使用的网络给改了，改成了他拿到的10.88.2.0/24
然后容器获得的ip就是10.88.2.0/24这个网段的IP
![[Pasted image 20240717215525.png]]
flannel用的时候，他还必须配合这另外一个软件来使用

<span style="background:#affad1">etcd</span>
和redis差不多，键值对的数据库
子网分配给了谁，都会在这个etcd这个数据库里面记录
记录网段的分配信息的
从网络的角度讲
左侧的某个容器要和右侧有个容器通信
最基本的他应该知道目的ip是99.0，那对于我左侧的物理机来说，他是怎么通的呢？他肯定要联系etcd查99.0这个网段被分配给了那个物理机，然后在通过这个数据库查询的结果把这个数据转发到右侧的这个物理机上
这个etcd相当于一个路由的角色

flannel网络的核心就是这个etcd数据库
K8S的核心也是etcd数据库
![[Pasted image 20240717220300.png]]

### 2.flannel网络部署
#### 2.1环境描述
192.168.140.22 docker/flannel/etcd  
192.168.140.23 docker/flannel
#### 2.2 两台物理机安装docker
#### 2.3 安装配置etcd数据库
yum install -y etcd
vim /etc/etcd/etcd.conf
	ETCD_LISTEN_CLIENT_URLS="http://0.0.0.0:2379" ETCD_ADVERTISE_CLIENT_URLS="http://0.0.0.0:2379"
systemctl enable --now etcd
netstat -tunlp | grep etcd
![[Pasted image 20240717220939.png]]
测试键值对数据库
基于文件的键值对
![[Pasted image 20240717221134.png]]
#### 2.4 安装配置flannel
安装flannel
yum install -y flannel
配置flannel
vim /etc/sysconfig/flanneld
FLANNEL_ETCD_ENDPOINTS="http://192.168.140.22:2379"
FLANNEL_ETCD_PREFIX="/atomic.io/network"

在etcd数据库中写入flannel网络信息
etcdctl mk  /atomic.io/network/config '{"Network":"10.88.0.0/16"}'

启动flannel
systemctl enable --now flanneld.service
ifconfig flannel0
![[Pasted image 20240717221840.png]]


配置flannel接管docker0
vim /usr/lib/systemd/system/docker.service
	ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock $DOCKER_NETWORK_OPTIONS
为什么写这个变量docker网络就变了呢？
就是配置好flannel这个软件后启动后生成的一个变量
这个变量存的就是flannel的子网信息
重启后docker的ip就变了
systemctl daemon-reload
systemctl restart docker
ifconfig docker0
![[Pasted image 20240717222838.png]]
查看一下那个变量在什么位置
cat /run/flannel/docker
![[Pasted image 20240717222911.png]]
另外一个服务器参考上述配置

### 测试容器通信
iptables -P FORWARD ACCEPT
![[Pasted image 20240717231324.png]]

## 八.docker镜像

### 1.镜像介绍
所有的容器都是要依赖于特定的镜像创建的，镜像如同模板一样(说法不够准确)
一个镜像就是一个<span style="background:#affad1">分层的文件系统</span>，并且<span style="background:#affad1">只读</span>
容器的镜像他都是由 多层文件系统叠加而成的，下载镜像的时候会一条一条的下载
![[Pasted image 20240718195530.png]]
这个就是分层的文件系统
![[Pasted image 20240718092507.png|350]]
从这个图上看 这个apache的镜像就是由三层文件系统构成的
最低层：表示容器和物理机共享一个内核(不包含在内)
三层文件系统：
最底层:debian系统，提供一些系统的基础指令(echo,ls.cd)等
上一层:提供emacs，这个就相当于debian系统里面的vim
最上层:提供apache相关的命令
这三层文件系统累加在一起，就算是构成一个<span style="background:#affad1">完整的apache镜像</span>
#### (1)<span style="background:#affad1">分层文件系统的</span><span style="background:#affad1">优势：</span>
节省空间，速度快，重用
比如：我们已经下载过apache的镜像，他的最低层是debian的文件系统，提ls，提供cd指令，当我们之后如果要下载tomcat的时候，有可能这个tomcat镜像底层也需要ls，cd指令，当我们下载tomcat镜像的时候，docker就能检测到我们这个机器里面已经有了一层文件系统，提供ls，提供cd了，那我们就可以直接复用了。
<span style="background:#affad1">只读</span>：任何一个容器镜像都是只读的

<font color="#ff0000">镜像是只读的？那为什么我们那这个镜像出来创建容器我们可以更改容器里面的额文件呢？镜像和容器有什么样子的关系？</font>
#### (2)镜像和容器的关系
有点类似于软件和进程之间的关系
在Docker中，镜像（Image）和容器（Container）之间的关系类似于面向对象编程中的类和实例。Docker <span style="background:#affad1">镜像是一个只读的模板</span>，<font color="#ff0000">它包含了运行一个容器所需的所有文件和配置</font>。镜像是不可变的，也就是说，一旦创建，其内容就不能再被修改。<span style="background:#affad1">容器是实例</span>：容器是从镜像创建的运行实例。容器可以运行、停止、移动和删除，它是镜像的实际运行状态镜像是静态的，它是一个包含了所有运行应用程序所需文件、依赖库、环境变量和其他配置的只读模板。容器则是基于这个镜像运行时的动态实例，它提供了一个执行环境，允许应用程序在其中运行。
#### (3)容器如何修改文件系统
<span style="background:#affad1">通过镜像创建容器，从镜像的角度来说就是相当于，在容器的最上方添加一个writable(可写成)，就是应为这个可写层的存在，我们在可以在容器里面对数据，对文件进行写操作</span>，这个层也被称之为<span style="background:#affad1">容器层(Container Layer)</span>，而这些修改不会影响到下面的镜像层。因此，即使容器内有文件被修改或新增，原始的镜像仍然保持不变。
敲docker run 这个命令的这一刻，就先当与在镜像上面加了一层可写层，把容器删了，这个可写层也就没了，所以跟大家说，跑关键性业务的时候，关键数据必须做持久化存储
总结：镜像就是一个死的文件系统，通过镜像添加容器，就是在这个镜像上面增加可写层，然后便可以称之为一个容器
#### (4)镜像是只读的原因
镜像是只读的，这样做的主要目的是为了确保其一致性、完整性和可重用性。只读特性使得镜像可以被多个容器安全地共享，而不会因为其中一个容器的修改而影响到其他容器。此外，只读属性还简化了镜像的分发和版本控制，因为它们不会被意外更改。

### 2.镜像核心技术
#### (1)cow 
copy on write 写时复制
只有发生写操作的收才会出现复制行为，逻辑卷也是支持写时复制功能的
从容器镜像的角度来讲
我们刚才说，镜像是一个死的文件系统，那这个镜像创建容器有个可写层，可以在可写层对文件数据进行修改
~~容器建出来之后，我会把镜像里面涉及到的所有文件全都给你放到可写层里面去，随时方便你修改~~（肯定不会是这样的！！！）效率太低
<span style="background:#affad1">正确的是：当你要改某一个文件的时候，我们才会把这个文件加载到内存里面或者加载到可写层里面来做修改！</span>
#### (2)Unionfs 
<span style="background:#affad1">联合文件系统</span>：支持多重文件系统的挂载
	overlay2：直通硬件
	device mapper：靠软件模拟实现
<font color="#ff0000">linux中常见的文件系统分三类</font>
	单机文件系统 ext4、Btrfs、XFS
	网络文件系统 nfs cifs
	集群文件系统 gfs  共享文件锁
第四类联合文件系统
从文件系统的角度讲，那镜像创建容器，就是相当于这么一回事，首先挂载第一层文件系统，第一层文件系统挂载出来之后，我们的容器里面相当于有了某些指令，挂完之后卸载，腾出空间再挂载第二层文件系统，以此类推
<span style="background:#affad1">镜像创建容器的过程，就是这些文件系统不停的挂载卸载的过程</span>
联合文件系统的挂载，不同于传统的挂载，文件系统挂载好之后，即使卸载了，相应的指令也还是会存在，并且可以叠加
<font color="#ff0000">linux的启动过程跟这个类似，先挂载内核，然后卸载，然后在挂载系统里面的各种初始化程序，卸载后再挂载上层应用</font>


### 3.镜像的核心操作
镜像文件存储路径
ls /var/lib/docker/image/overlay2/layerdb/mounts
#### (1)查看镜像列表
docker image ls 
![[Pasted image 20240715113429.png]]
TAG：标记镜像的版本
#### (2)搜索镜像
docker search nginx
#### (3)下载镜像
docker pull nginx:1.18
#### (4)导入镜像
docker load -i centos7.tar
#### (5)导出镜像
docker save -o tomcat.tar tomcat:latest
#### (6)删除镜像
docker rmi id 
#### (7)查看镜像详情
docker image inspect centos:7
查看镜像的详细信息，找Cmd的关键字
#### (8)更改镜像名称
docker tag 555308ffc1d7 suiyi:1003

## 九.Dockerfile定制镜像
### 1.Dockerfile使用教程

#### (1)编写dockerfile
vim dockerfile.txt
FROM centos:7
COPY CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo
RUN yum -y install net-tools
#### (2)构建镜像
docker build -t centos:v1  ./ -f dockerfile.txt
#### (3)创建容器测试镜像定制操作
docker image ls
![[Pasted image 20240718141029.png]]
docker run -tid --name=test1 centos:v1
docker exec -ti test1 bash
ifconfig

### 2.dockerfile 常用指令
#### -FORM
指定基础镜像
镜像不村子，构建镜像时自动下载镜像
建议尽量选择小容量的镜像 /debian/ubuntu
FROM 镜像名称

#### -RUN
指定定制命令
RUN 命令   &&  命令   &&  命令

#### -CMD
定义容器创建时自动执行的命令
注意事项
前台启动服务的指令
创建容器时，不要自己指定命令，会覆盖CMD
一个dockerfile只能有一条CMD指令
CMD 命令
CMD http -D FOREGROUNG
CMD ['HTTPD','-D','FOREGROUNG']推荐

#### -ENTRYPOINT
定义容器创建时，自动执行的命令
ENTRYPOINT ['HTTPD','-D','FOREGROUNG']
CMD写的指令会被覆盖；ENTRYPOINT不会被覆盖
ENTRYPOINT 在复杂的场景会有特殊的应用
定制相对复杂的镜像的时候，写脚本通过ENTRYPOINT执行脚本

#### -COPY 
复制文件
COPY 源文件 目的文件
只能复制本地文件

#### -ADD
复制文件
ADD 源文件目的文件
支持本地文件，还支持远程的url 相当于wget
拷贝压缩包的化，压缩包会自动解压，只支持本地的压缩文件

#### -EXPOSE
说明容器服务端口
EXPOSE 端口 端口
注意：
-P随机发布端口时，dockerfile中必须有EXPOSE指令
EXPOSE 80/TCP

#### -VOLUME
定义持久化存储的目录
创建容器的时候不使用-v明确指定目录，会自动生成匿名卷
VOLUME 目录
RUN mkdir /data
VOLUME /data

#### -ENV
定义环境变量
ENV 环境变量名称 值

#### -WORKDIR
定义当前目录
WORKDIR /data
-USER
指定容器运行的用户
USER 用户名

### 3.练习
编写tomcat镜像

## 十.微服务项目的部署
192.168.183.10
微服务项目部署要按顺序
第一个容器
![[Pasted image 20240719101542.png]]
![[Pasted image 20240719101602.png]]

数据库容器
![[Pasted image 20240719101708.png]]
![[Pasted image 20240719101801.png]]

登录数据库导入三个库
![[Pasted image 20240719101840.png]]
登录数据库确认一下
![[Pasted image 20240719101903.png]]
创建远程连接的账号
![[Pasted image 20240719102033.png]]

创建消息队列容器
![[Pasted image 20240719102145.png]]
![[Pasted image 20240719102153.png]]

查看端口5672，前端业务连接 15672web管理界面
![[Pasted image 20240719102209.png]]
![[Pasted image 20240719102250.png]]
能成功访问说明rabbitmq 是部署成功的
guest guest 登录
需要在消息队列里面创建业务连接的账号
![[Pasted image 20240719102406.png]]
![[Pasted image 20240719102416.png]]
![[Pasted image 20240719102438.png]]
给用户授权
![[Pasted image 20240719102455.png]]
![[Pasted image 20240719102715.png]]
项目部署底层的三个组件ok

部署微服务框架
1.注册中心
2.配置中心
3.网关

部署erueka
注册中心，负责注册所有服务的通信地址
自己定义eureka的镜像
vim eurekadockerfile
![[Pasted image 20240719103918.png]]
**
![[Pasted image 20240719104009.png]]
![[Pasted image 20240719104045.png]]
查看是否正确启动
![[Pasted image 20240719104203.png]]
![[Pasted image 20240719104226.png]]

部署config-server
配置中心
负责集中管理所有业务组件的配置文件
把所有的配置集中放到git上面
从远程的git服务器上拉去配置
vim configdockerfile
![[Pasted image 20240719104835.png]]
![[Pasted image 20240719104901.png]]
![[Pasted image 20240719104928.png]]

查看日志看是否正常启动
![[Pasted image 20240719105023.png]]
别有error的报错

部署zuul-server
网关
从开发的角度来说
这个网关是用来集中处理请求的，类似于反向代理
![[Pasted image 20240719105304.png|325]]
vim zuuldockerfile
![[Pasted image 20240719105506.png]]
![[Pasted image 20240719105523.png]]

![[Pasted image 20240719105644.png]]
检查是否可以正常运行

部署业务
部署会员服务
vim memberdockerfile
![[Pasted image 20240719112308.png]]
![[Pasted image 20240719112318.png]]
![[Pasted image 20240719112634.png]]
查看日志
![[Pasted image 20240719112703.png]]

部署商品展示的业务
vim gooddockerfile
![[Pasted image 20240719112829.png]]
![[Pasted image 20240719112849.png]]
所有业务容器的主机名都是localhost
![[Pasted image 20240719112942.png]]
docker logs good-server

部署秒杀业务
参考上述流程
vim seckilldockerfile
![[Pasted image 20240719113133.png]]

![[Pasted image 20240719113158.png]]
![[Pasted image 20240719113242.png]]
docker logs seckill-server
![[Pasted image 20240719113312.png]]

部署web前端
提供前端的web页面
参考上述流程
vim frontenddockerfile
构建镜像
创建容器
查日志保证正常运行

部署websocket
实现http的长链接
vim websockerdockerfile
构建镜像
创建容器
查日志保证正常运行

![[Pasted image 20240719113914.png]]
docker logs websocket
8088端口

流程不能错！！！！！

访问前端测试
192.168.183.10:8080
![[Pasted image 20240719114232.png]]




## 十一.基于harbor镜像仓库
存放镜像的位置
### 1.仓库的类型
公有仓库           私有仓库:企业级的应用
[Dockerhub](https://hub.docker.com/)    registry;harbor(公司内部局域网)
其他的服务器再创建容器的时候需要镜像可以直接从公司内部的仓库下载
(1)走局域网速度快
(2)公司有新项目上线的话，新的定制的镜像也可以集中上传到这个仓库服务器
### 2.构建私有仓库的方案
registry(早期;容器镜像):就是一个容器;功能太少,缺少日志,权限分配,高可用功能;仅支持上传下载;5000端口
<span style="background:#affad1">harbor</span>(软件):VMware开源的;在早期的registry的基础之上进行改进;增加了web ui;支持日志审计;权限分配;统一认证等功能，更贴合公司实际的使用需求
两种安装包:离线安装包(包含镜像)；在线安装包(内存小;不包含镜像;需要自己下载)

### 3.部署harbor仓库(单机版)
#### 3.1安装docker
#### 3.2安装docker-compose工具
支持批量创建容器，和批量删除容器
mv docker-compose /usr/local/bin/ 
就是个命令所以把包放到系统的命令目录里就可以了
docker-compose version
![[Pasted image 20240722190736.png]]
#### 3.3安装harbor
```bash
mkdir /work
tar xf harbor-offline-installer-v2.2.2.tgz -C /work/
cp /work/harbor/harbor.yml.tmpl /work/harbor/harbor.yml # 复制配置文件
```
**修改配置文件**
![[Pasted image 20240722191106.png]]
harbor可以支持通过http协议工作，默认端口80;也支持https默认用的端口是443，<span style="background:#affad1">但是我们现在用harbor都是用https协议</span>；这个docker有关:仓库搭建好以后，想要上传或者下载镜像，还是需要用docker命令，而docker内部默认的就是基于https的，如果你的仓库设置成http的docker的配置还要改，所以我们仓库一般都用https
#### 3.4生成harbor需要的证书;密钥(v3版本)
harbor的版本从2.2开始就是一个分割线；2.1版本之前harbor需要证书密钥我们可以那我们之前学的https拿opens命令生成证书和密钥就可以了；但是2.2版本之后，harbor需要的证书必须是v3版本(第三个版本)的证书
##### (1)创建CA
生成个CA需要的密钥
```bash
mkdir /opt/ssl
cd /opt/ssl
openssl genrsa -out ca.key 4096 
```
![[Pasted image 20240722192216.png]]
生成个CA需要的证书
```bash
openssl req -x509 -new -nodes -sha512 -days 3650  -subj "/CN=harbor.linux.com"  -key ca.key  -out ca.crt  # 生成个CA需要的证书
ls
```
![[Pasted image 20240722192242.png|675]]
##### (2)创建harbor仓库需要的证书
创建harbor仓库需要的密钥
```bash
openssl genrsa -out server.key 4096
```
![[Pasted image 20240722192444.png|700]]
申请一个harbor仓库需要的证书
```bash
openssl req  -new -sha512  -subj "/CN=harbor.linux.com"  -key server.key  -out server.csr
vim v3.ext   # 真正生成证书的时候需要这么一个文件
	authorityKeyIdentifier=keyid,issuer
	basicConstraints=CA:FALSE
	keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
	extendedKeyUsage = serverAuth 
	subjectAltName = @alt_names
	[alt_names]
	DNS.1=harbor.linux.com
```
生成证书
```
openssl x509 -req -sha512 -days 3650 -extfile v3.ext -CA ca.crt -CAkey ca.key -CAcreateserial -in server.csr -out  server.crt
```
![[Pasted image 20240722192348.png]]
ca.key      密钥
server.csr 证书申请
server.crt  证书】
#### 3.5更改harbor配置文件
vim  harbor.yml
![[Pasted image 20240722193922.png|550]]
默认自带admin管理员用户
![[Pasted image 20240722193803.png|550]]
所有的仓库必须做持久化,默认目录就是/data
![[Pasted image 20240722194050.png|550]]
#### 3.6启动harbor
启动的时候会下载prepare镜像,我们可以提前导入
docker load -i  prepare.tar 
在harbor目录下
./prepare
运行prepare脚本主要是要干啥呢？
这个harbor仓库在运行的时候需要用nginx做反向代理，redis做会话，需要后台数据量来存储数据，然后需要很多的组件，所以这个脚本就是为了生成个个组件的配置文件
<span style="background:#affad1">然后会自动生成docker-compose.yml文件</span>
ls /harbor/common/config/  # 这个目录下
启动harbor
./install.sh 
![[Pasted image 20240722195016.png|400]]
docker image ls 
![[Pasted image 20240722195046.png|700]]
要确保这九个容器都启动;并且都是健康的状态
访问web
![[Pasted image 20240722200222.png]]
默认账号admin 密码Harbor12345

### 4.harbor仓库的使用
#### 4.1创建项目
![[Pasted image 20240722201635.png|325]]
![[Pasted image 20240722201707.png|475]]
#### 4.2创建用户授权
![[Pasted image 20240722202320.png|400]]
![[Pasted image 20240722202351.png|525]]
把用户添加到项目里面
![[Pasted image 20240722202700.png|500]]
#### 4.3上传镜像
##### (1)登录仓库
docker login # 回车默认登录dockerhub
docker login harbor.linux.com  # 需要添加解析
docker服务器默认走443端口，需要证书认证，需要再docker服务器上面添加证书
```bash
mkdir /etc/docker/certs.d/harbor.linux.com -p
scp root@192.168.140.11:/opt/ssl/server.crt /etc/docker/certs.d/harbor.linux.com/
docker login harbor.linux.com
```

##### (2)为镜像打标记
docker tag  （类似于给镜像改名字）
镜像的命名格式:
仓库名称/镜像名称:标记
docker tag websocket-server:1.0 harbor.linux.com/miaosha/websocket-server:1.0
##### (3)上传镜像
docker push harbor.linux.com/miaosha/websocket-server:1.0 
#### 4.4退出仓库
docker logout harbor.linux.com



### 5.harbor核心组件
单个harbor很容易成为单点故障
![[Pasted image 20240722150812.png|650]]
core services
提供web ui
令牌token
与registry交互获取镜像的元数据信息在webUI上展示
 - ui
 - token 给用户生成token令牌持续保存登录状态
 - webhook(钩子函数) 和 registry做交互
registry(最核心)
负责镜像的上传下载
Log collector
采集日志
Job service (高可用组件)
负责在多个harbor仓库间同步数据
Proxy
使用nginx对后端所有组件进行反向代理

Database
	redis：存储前端用户产生的令牌
	harbor-db：关系型数据库，存放harbor仓库上数据；默认是postgreSQL/pgSQL


### 6.harbor高可用设计方案
核心思想
禁用其自带的数据库，配置连接第三方的库， 保证多个harbor间的数据同步
![[Pasted image 20240722205030.png|475]]
192.168.183.10 harbor仓库  
192.168.183.20 harbor仓库  
192.168.183.30 后端数据库、存储

#### 6.1配置nfs作为harbor的数据目录
yum -y install nfs-utils
vim /etc/exports
/harbor_data	192.168.183.10(rw,no_root_squash) 192.168.183.20(rw,no_root_squash)
systemctl enable --now nfs-server
#### 6.2安装redis
作为harbor共享缓存
docker run -tid --name=harbor_redis --net=host --restart=always redis:latest 
#### 6.3安装postgreSQL
作为harbor的共享数据库
mkdir -p /pgsql/data
docker run -tid --name=harbor_pgsql -e POSTGRES_PASSWORD=redhat -e PGDATA=/var/lib/postgresql/data/pgdata -v /pgsql/data:/var/lib/postgresql/data/pgdata --net=host --restart=always postgres:12.2 

配置数据库
docker exec -ti harbor_pgsql bash
psql -h 127.0.0.1 -p 5432 -U postgres
create user harbor with password 'redhat';
create database harbor;
create database harbor_clair;
create database harbor_notary_server;
create database harbor_notary_signer;
grant all on database harbor to harbor;
grant all on database harbor_clair to harbor;
grant all on database harbor_notary_server to harbor;
grant all on database harbor_notary_signer to harbor;
修改pgSQL的配置文件，允许远程主机(harbor仓库)连接
cd /var/lib/postgresql/data/pgdata
echo "host all all all trust" >> pg_hba.conf 

#### 6.4两台harbor仓库挂载nfs存储作持久卷
tail -n 1 /etc/fstab
192.168.140.13:/harbor_data	/data	nfs	defaults	0 0
mount -a 
#### 6.5编辑harbor配置文件
 禁用自带数据库，连接外部数据库
 vim harbor.yml
 ```bash title:harbor.yml 
 # Harbor DB configuration
# database:
  # The password for the root user of Harbor DB. Change this before any production use.
  # password: root123
  # The maximum number of connections in the idle connection pool. If it <=0, no idle connections are retained.
  # max_idle_conns: 50
  # The maximum number of open connections to the database. If it <= 0, then there is no limit on the number of open connections.
  # Note: the default number of connections is 1024 for postgres of harbor.
  # max_open_conns: 1000

// 配置harbor连接外部的pgsql
external_database:
  harbor:
    host: 192.168.140.10
    port: 5432
    db_name: harbor
    username: harbor
    password: redhat
    ssl_mode: disable
    max_idle_conns: 2
    max_open_conns: 0
  notary_signer:
    host: 192.168.140.10
    port: 5432
    db_name: harbor_notary_signer
    username: harbor
    password: redhat
    ssl_mode: disable
  notary_server:
    host: 192.168.140.10
    port: 5432
    db_name: harbor_notary_server
    username: harbor
    password: redhat
    ssl_mode: disable

//配置harbor连接外部redis
external_redis:
#   # support redis, redis+sentinel
#   # host for redis: <host_redis>:<port_redis>
#   # host for redis+sentinel:
#   #  <host_sentinel1>:<port_sentinel1>,<host_sentinel2>:<port_sentinel2>,<host_sentinel3>:<port_sentinel3>
  host: 192.168.140.10:6379
#   password:
#   # sentinel_master_set must be set to support redis+sentinel
#   #sentinel_master_set:
#   # db_index 0 is for core, it's unchangeable
  registry_db_index: 1
  jobservice_db_index: 2
  chartmuseum_db_index: 3
  trivy_db_index: 5
  idle_timeout_seconds: 30

```
#### 6.6启动harbor
```bash
./prepare
./install.sh 
另外一台harbor仓库配置参考上述
scp /usr/local/bin/docker-compose root@192.168.183.20:/usr/local/bin/docker-compose
docker-compose version
scp -r /opt/ssl* root@192.168.183.10:/opt/ssl
scp -r /work/harbor/ root@192.168.183.10:/work/
scp ~/prepare.tar root@192.168.183.10:~/
docker load -i prepare.tar
scp -r /work/harbor/ root@192.168.183.10:/work/harbor/
```
#### 6.7配置haproxy做harbor仓库的负载均衡
vim /opt/work/haproxy.cfg
```bash
frontend harbor
   bind 0.0.0.0:9443
   mode tcp
   use_backend harbor_server
backend harbor_server
   mode tcp
   balance roundrobin
   server harbor01 192.168.183.10:443
   server harbor02 192.168.183.20:443
```
docker run -tid --name=harbor_haproxy -p 443:9443 -v /opt/work/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg --restart=always haproxy:latest 
客户端测试通过haproxy访问仓库 

## 十二.docker-compose
### 1.docker-compose安装
单机版的容器编排工具
```bash
curl -L "https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose 
mv docker-compose /usr/local/bin/ 
chmod a+x /usr/local/bin/docker-compose
```
查看docker-compose版本
docker-compose version 
批量操作容器
docker-compose.yml
docker-compose up -d
docker-compose down  
k8s也是一个容器编排工具
多个不同机器上的docker统一编排，就需要集群版的容器编排工具<span style="background:#affad1">k8s</span>
### 2.docker-compose使用流程
![[Pasted image 20240723092736.png]]
没有yml文件情况的报错
version: '3'     docker-compose和docker-api对接的版本，真正的删除创建操作本质上还是docker做的docker-compose只是调用docker
services:        下面写你要创建的容器信息
![[Pasted image 20240723093827.png|625]]
docker-compose up -d
![[Pasted image 20240723093858.png]]
会默认创建一个网络(nat类型)
docker ps -a
![[Pasted image 20240723094003.png]]
注意命名规则：当前目录_services名称_数字
### 3.docker-compose常用的指令
#### 3.1整体结构
```yaml
version: '3'
services:
    服务名称:
         容器选项:
    服务名称:
         容器选项:   
networks: 定义容器网络信息
volumes: 定义数据卷
```
#### 3.2常用选项
```bash title:常用选项
image: 镜像名称
volumes: 
	- 物理目录:容器目录 
	- 物理目录:容器目录 
command: 
	-"shell命令"
	-"shell命令"
```
![[Pasted image 20240723101800.png]]
docker run 创建能够默认允许是应为，-t -i 提供了一个终端
没有终端怎么执行/bin/bash
这就是docker-compose编排创建的时候，centos:7的容器虽默认执行bash但是无法启动的原因
```bash title:常用选项
links: - 容器名称:别名              # 容器名称写的是服务名称
ports:                                        # 发布容器服务
	- 物理机端口:容器端口 
	- 物理机端口:容器端口 
expose: - 端口                          # 指定容器的服务端口
environment:  
	key: value 
	key: value
depends_on:                             # 定义容器依赖关系,启动顺序
	- 容器名称 
	- 容器名称
depoly:                                      # 定义容器的副本数
	replicas: 3 
    resources:                         # 做资源限制
		limits:                        # 硬限制 最高资源
			cpus: "0.50" 
			memory: 50M 
		reservations:            # 软限制 最低资源
			cpus: "0.25" 
			memory: 20M
healcheck:                                # 容器进行健康状态检测
	test: ["CMD", "curl", "-f", "http://localhost"] 
	interval: 10s 
	timeout: 10s 
	retries: 3 
	start_period: 40s              # 定义健康状态检查开始的时间
networks_mode: 'host'             # 定义容器网络模式
```
### 4.配置nginx负载均衡案例
nginx tomcat tomcat

# Kubernetes
## 一.Kubernetes概述
### 1.kubernetes是什么
有谷歌公司基于go语言开发的集群版的容器编排工具
简称K8s，K8s就是一个集群
管理容器的工具 docker docker-compose kubernetes 
三五个机器部署一套k8s集群出来，所有的容器都是创建在集群里面的
K8s官网：kubernetes.io
<span style="background:#affad1">用K8s管理容器就是为了让容器的管理变得更简单更加高效</span>

### 2.Kubernetes作用/优势
#### (1)自我修复
一旦检测到我的容器挂了以后，他会自动给我新建出一个一摸一样的容器出来(一摸一样！)
如果坏的是数据库呢?
数据库最重要的是数据，K8s针对数据库挂了，会自动创建一个新的数据库容器，然后自动连接数据存储设备
#### (2)滚动更新
分批次更新
假设业务服务器有四个，分批次更新就是可能先把新版本更新到两个服务器上，让新版本先运行一段时间，没错后，在接着更新其余的业务服务器，这样子就算新版本有bug也不会影响所有的终端用户
#### (3)支持服务发现和负载均衡
自动做负载均衡
四个容器部署好了以后，跑一样的业务，传统我们肯定要通过haproxy或者ngxin做负载均衡，但是有了K8s以后， 只要你创建了多个一莫一样的容器，k8s就会在这个多个容器上自动做负载均衡，k8s集群接收到用户的访问量之后，他会把这个访问量自动分担到相同的容器上去。
他在做负载均衡的时候，本质上k8s调用的就是<span style="background:#affad1">linux内核里面的lvs</span>
#### (4)支持存储编排
也就是说K8s支持多种类型的存储
#### (5)支持水平扩展
做一些阈值的控制，提升资源的利用率
例如：只要cpu的是使用率超过了百分之70%,会自动的去帮我扩展这个容器的数量，等着cpu的使用率降下去了，就会自动的删除多余的容器

总而言之，就是让容器化的应用部署变得更简单，高效
现在很多大厂都对K8s做了二次封装
kubernetes和docker有什么区别？
## 二.kubernetes核心组件
![[Pasted image 20240723152208.png]]
### 1.核心节点
<font color="#f79646">master节点</font>(主节点)
负责正整个集群的管理操作(集群状态的监控，维护，扩容，缩容)
<font color="#f79646">node节点</font>(工作节点)
至少一个node节点，多了不限，运行容器的节点
<font color="#f79646">etcd数据库节点</font>  
高性能的键值对数据库，作为K8s集群的后台数据库使用，存储集群的所有数据
至少是一个三节点的高可用集群，etcd节点是k8s的核心，要按时做备份
<font color="#4bacc6">etcd数据库怎么做备份(以快照的方式 )？</font>
<font color="#f79646">kubectl</font>
客户端命令行工具和docker类似
### 2.核心组件 | <span style="background:#affad1">面试必聊</span>
Kubernetes 1.25 版本开始，不再依赖 Docker 作为容器运行时。使用containerd
#### 2.1 Master节点的组件 
说是自建，启动的时候就是一个一个的进程
<font color="#f79646">kube-apiserver</font>
(1)负责接收客户端操作请求、认证授权(安全角度)
(2)负责与etcd数据库交互，~~K8s所有的数据都是在etcd里面~~，K8s里面所有的数据都是要存在etc里面，谁存进去的？<font color="#f79646">apiserver这个组件</font>
我们想查看所有容器的状态，但是我们能看到的额所有数据都是从etcd的数据库里面来的，那谁帮我们去etcd数据库里面取数据呢？<font color="#f79646">apiserver这个组件</font>
(3)负责接收工作节点的注册请求
apiserver接收到一个创建容器的请求，这个请求肯定是要创建在某一个工作节点上，假设你是master节点，你是怎么知道你的集群里面有哪些工作节点，这些工作节点的状态是不是都是好的呢？然后你还需要知道工作节点的ip,知道这些信息以后最终才能找到工作节点，才能把真正的容器创建出来。
我们在搭建集群的时候，首先我们要先把这个主节点搭好，然后再去部署工作节点，工作节点上的服务(kublet)起来以后，他会主动练习apiserver注册自己，把自己的各种信息告诉apiserver，然后apiserver把这些信息存入到etcd数据库
<font color="#f79646">kube-scheduler 调度器</font>
选择一个合适的工作节点来运行容器
<font color="#f79646">kube-controller-manager 控制器管理器</font>
是一个编译好的二进制程序，他把所有的控制器全都编译好编辑进去了；负责管理k8s集群内部中所有的控制器的(无状态、有状态的容器等)
#### 2.2 Node节点的组件
<font color="#f79646">容器管理引擎</font>
可以是docker, containerd
以前k8s和docker对接的时候k8s产生的数据格式底层docker不支持
所以以前k8s和底层docker进行配合的时候中间需要有个转换，必须有个翻译所以影响效率，所以k8s从1.25版后弃用docker，改用containerd，性能效率考虑
<font color="#f79646">kubelet</font>
负责调用工作节点的容器引擎进行容器的整个生命周期管理(增删改查)；向api server发送注册请求
与openstack里面的nova-computer是一样的
<font color="#f79646">kube-proxy</font>
负责服务发布、负载均衡(调用Linux内核的lvs)
一些复杂的业务，可能还会涉及到物理机上面写一些防火墙规则，做一些数据过滤



## 三.k8s创建容器的流程
<font color="#f79646">1.创建容器的请求首先发给apiserver</font>
apiserver接收到我们创建容器的请求，对我们进行身份验证，鉴权通过之后，他来接收我们创建容器的请求
apiserver拿到我们创建容器的请求之后，他肯定能够知道我们需要创建的容器的各种信息，
<font color="#f79646">2.然后apiserver会把容器的创建信息写道etcd数据库里面去</font>
比如存着：<span style="background:#affad1">容器名xxxx 镜像名xxx (创建请求)</span>
然后这个时候schedule这个调度会干什么呢？
<font color="#f79646">3.schedule会周期性的询问apiserver你有没有请求需要我调度</font>，apiserver接收到这个询问之后，他就回去etcd数据库里面去查，肯定能够查到数据库里面存在着一个创建请求，然后他会拿着这个请求返回告诉给这个调度
调度现在就知道了有一个创建的请求需要调度
<font color="#f79646">4.然后schedule就会按着一定的算法选择一个工作节点</font>，选择到合适的工作节点之后
<font color="#f79646">5.然后scheduler会把这个工作节点的ip和端口告诉apiserver</font>
说我帮你选择一个容器，apiserver拿到这个工作节点的ip等信息后，<font color="#f79646">6.apiserver又会把这些信息存放到etcd数据库里面，</font>会跟之前的创建请求形成一个对应关系
<span style="background:#affad1">容器名xxxx 镜像名xxx (创建请求)</span><span style="background:rgba(136, 49, 204, 0.2)">--->node02</span>

<font color="#f79646">7.工作节点的kubelet会不停的找apiserver去问，你这边有没有需要我真正帮你创建的容器，</font>
apiserver接收到询问之后，又回去etcd库里面去查有没有 <span style="background:#affad1">容器名xxxx 镜像名xxx (创建请求)</span><span style="background:rgba(136, 49, 204, 0.2)">--->node02</span>这样子的创建关系存在，
<font color="#f79646">8.apiserver查到相应的信息之后，会把这个信息返回给kubelet</font>
kubelet拿到apiserver返回的信息之后
<font color="#f79646">9.这个时候kubelet才会调用底层的Docker Engine 最终把这个容器创建出来</font>
创建出来之后kubelet再把建好的容器信息返回给apiserver，
<font color="#f79646">10.然后apiserver再把信息存到etcd数据库里</font>
k8s底层设计的时候，为什么不用关系性数据库呢？
应为Nosql类型的数据库速度要比关系型数据库快的多得多

## 四.部署K8s1.29集群
环境规划
192.168.140.21 K8s-master.linux.com     2u4G
192.168.140.22 K8s-node01.linux.com    2u4G
192.168.140.23 K8s-node02.linux.com    2u4G
### 1.主机环境配置
（所有的主机都要做）
#### 1.1 关闭防火墙selinxu时间同步 
```bash
ntpdate ntp.aliyun.com
crontab -e
*/30 * * * * /usr/sbin/ntpdate ntp.aliyun.com &> /dev/null
crontab -l
```
#### 1.2 所有主机配置ssh免密
```bash
ssh-keygen -t rsa
mv /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys
scp -r /root/.ssh/ root@192.168.140.22:/root/
scp -r /root/.ssh/ root@192.168.140.23:/root/
```
#### 1.3主机名解析
192.168.140.21 k8s-master.linux.com
192.168.140.22 k8s-node01.linux.com
192.168.140.23 k8s-node02.linux.com
```bash
for i in 22 23  
> do 
> scp /etc/hosts root@192.168.140.$i:/etc/hosts 
> done
```
#### 1.4 所有主机关闭swap
free -m    # 查看swap
![[Pasted image 20240724200253.png]]
```bash
关闭swap
swapoff -a                           临时关闭
sed -ri '/swap/d' /etc/fstab  永久关闭
free -m
```
![[Pasted image 20240724200333.png]]

#### 1.5 所有主机修改内核参数
```bash
vim /etc/sysctl.d/k8s.conf
	net.bridge.bridge-nf-call-iptables=1
	net.bridge.bridge-nf-call-ip6tables=1
	net.ipv4.ip_forward=1
	vm.swappiness=0
	vm.overcommit_memory=1
	vm.panic_on_oom=0
	fs.inotify.max_user_instances=8192
	fs.inotify.max_user_watches=1048576
	fs.file-max=52706963
	fs.nr_open=52706963
	net.ipv6.conf.all.disable_ipv6=1
	net.netfilter.nf_conntrack_max=2130720
	25:24 老师讲解了部分参数的意思
```
sysctl -p /etc/sysctl.d/k8s.conf
加载内核模块
modprobe br_netfilter
modprobe ip_conntrack


#### 1.6 安装基础软件依赖
yum install -y wget jq psmisc net-tools nfs-utils socat telnet device-mapper-persistent-data lvm2 git tar zip curl conntrack ipvsadm ipset iptables sysstat libseccomp

#### 1.7 加载lvs负载均衡策略
vim  /etc/sysconfig/modules/ipvs.modules
```bash
#!/bin/bash
#
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack
```
bash /etc/sysconfig/modules/ipvs.modules
lsmod | grep -e ip_vs -e nf_conntrack
![[Pasted image 20240724201453.png]]

### 2.安装容器虚拟引擎软件
所有主机
k8s 1.25版本后弃用docker， 改用containerd
#### 2.1安装docker-ce
vim /etc/yum.repos.d/docker-ce.repo
```bash title:docker-ce.repo
[docker-ce-stable]
name=Docker CE Stable - $basearch
baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/$basearch/stable
enabled=1
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg

[docker-ce-stable-debuginfo]
name=Docker CE Stable - Debuginfo $basearch
baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/debug-$basearch/stable
enabled=0
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg

[docker-ce-stable-source]
name=Docker CE Stable - Sources
baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/source/stable
enabled=0
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg

[docker-ce-test]
name=Docker CE Test - $basearch
baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/$basearch/test
enabled=0
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg

[docker-ce-test-debuginfo]
name=Docker CE Test - Debuginfo $basearch
baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/debug-$basearch/test
enabled=0
gpgcheck=1
yum install -y docker-cegpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg

[docker-ce-test-source]
name=Docker CE Test - Sources
baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/source/test
enabled=0
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg

[docker-ce-nightly]
name=Docker CE Nightly - $basearch
baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/$basearch/nightly
enabled=0
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg

[docker-ce-nightly-debuginfo]
name=Docker CE Nightly - Debuginfo $basearch
baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/debug-$basearch/nightly
enabled=0
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg

[docker-ce-nightly-source]
name=Docker CE Nightly - Sources
baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/source/nightly
enabled=0
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg

```

```bash
yum install -y docker-ce
vim /etc/docker/daemon.json
{
  "registry-mirrors": ["https://rywdmoco.mirror.aliyuncs.com"]
}
systemctl enable --now docker
```
#### 2.2 安装containerd
yum install -y containerd.io 
生成一份默认的配置文件
```bash
containerd config default > /etc/containerd/config.toml
vim /etc/containerd/config.toml 
SystemdCgroup = true
sandbox_image = "registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.6"
systemctl enable --now containerd.service 
```
#### 2.3 安装kubeadm/kubectl/kubelet软件
```bash
vim  /etc/yum.repos.d/k8s.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.29/rpm/
enabled=1
gpgcheck=0
```
yum install -y kubeadm-1.29.1 kubectl-1.29.1 kubelet-1.29.1 

#### 2.4 启动kubelet
vim  /etc/sysconfig/kubelet
KUBELET_EXTRA_ARGS="--cgroup-driver=systemd"
systemctl enable --now kubelet

#### 2.5 配置crictl客户端工具
```bash
vim  /etc/crictl.yaml
	runtime-endpoint: unix:///run/containerd/containerd.sock
	image-endpoint: unix:///run/containerd/containerd.sock
	timeout: 10
	debug: false
systemctl restart containerd.service 
```

### 3.部署k8s集群
#### 3.1 在Master节点创建集群初始化文件
```bash
kubeadm config print init-defaults > init.yaml
advertiseAddress: 192.168.140.10 # kube-apiserver监听地址
name: k8s-master.linux.com           # master节点的名称， 控制面板
imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers # 镜像仓库地址
kind: ClusterConfiguration
kubernetesVersion: 1.29.1		         # k8s集群版本，和kubeadm版本一致
networking:
  dnsDomain: cluster.local
  podSubnet: 10.88.0.0/16	         # pod网段，给容器分配IP的网段
  serviceSubnet: 10.96.0.0/16
```
#### 3.2 初始化集群
建议提前导入相应的镜像, docker导入的镜像是在文件系统中(磁盘)  
containerd导入的镜像在k8s.io命名空间
ctr -n k8s.io image import xxxxx.tar 
master节点
![[Pasted image 20240725091816.png]]
![[Pasted image 20240725091850.png]]
node节点
![[Pasted image 20240725091914.png]]

kubeadm init --config=init.yaml  --ignore-preflight-errors=SystemVerification
![[Pasted image 20240725092007.png]]
#### 3.3 定义KUBECONFIG环境变量
目的：为了让kubectl客户端工具可正常使用
vim /etc/profile
export KUBECONFIG=/etc/kubernetes/admin.conf
source /etc/profile
#### 3.4 添加工作节点
在工作节点执行
![[Pasted image 20240725092316.png]]
![[Pasted image 20240725092346.png]]
#### 3.5 查看初始化后的状态
![[Pasted image 20240725092417.png]]
![[Pasted image 20240725092438.png]]
### 4.部署calico打通容器网络通信
calico基于BGP协议设计
```bash
vim calico.yaml 
 - name: CALICO_IPV4POOL_CIDR
    value: "10.88.0.0/16"
```
这个calico.yaml文件从老师ftp下载的
事先导入镜像
```bash
ctr -n k8s.io image import calico_node_v3.27.0.tar 
ctr -n k8s.io image import calico_kube-controllers_v3.27.0.tar 
ctr -n k8s.io image import calico_cni_v3.27.0.tar 
```
kubectl create -f calico.yaml 

### 5.k8s集群部署完成
#### 5.1查看核心组件运行状态
![[Pasted image 20240725092848.png]]
#### 5.2 查看节点的运行状态
![[Pasted image 20240725092924.png]]

## 五. Kubernetes资源-pod
工作负载
### 1.namespace 命名空间
#### 1.1 命名空间介绍
docker使用命令空间只要是来实现资源隔离的
在K8s中也是用来实现各种不同类型资源之间的隔离的，通过这个命名空间可以对资源进行分组
不同业务的容器可以放置在不同的命名空间里面，方便管理

k8s搭建之后会默认提供四个命名空间
kubectl get ns  # 查看命名空间
![[Pasted image 20240725184034.png]]
如果在创建资源的时候我们不明确指定命名空间的话，默认使用的就是default这个命名空间
kubectl get pod
![[Pasted image 20240725184308.png]]
kubectl get pod -n kube-system  # -n 指定命名空间
kubectl get pod -A # 查看所有命名空间里的所有pod
![[Pasted image 20240725184525.png]]
#### 1.2 创建命名空间
##### (1)通过命令行创建
kubectl create ns game  
##### (2)通过yaml文件的方式(编排)
vim web.yaml
```bash
apiVersion: v1             # api版本
kind: Namespace       # 资源类型
metadata:                   # 元数据
	name: web
```
kubectl create -f web.yaml
k8s支持的资源类型很多，支持namespace，有状态的容器，无状态的容器，支持服务，支持存储，支持配置映射，每种资源对应一个api接口的版本
kubectl delete -f web.yaml  # 删除这个yaml文件里描述的资源

### 2.pod 
#### 2.1.pod介绍
非常重要的一类资源
后面我们在实际使用K8s的时候，很少直接操作pod本身，但是后续k8s后续的资源都和pod有关
一个pod可以当作一个容器(但是一个pod可以放多个容器)
<font color="#f79646">pod是k8s集群所能够管理的最小单位</font>
<font color="#f79646">相当于装载容器的箱子</font>
<font color="#f79646">实际情况：一个容器一个pod</font>
K8s搭好之后就是为了借助这个集群管理容器，但是K8s这个容器它设计的时候不会直接操作容器
<span style="background:#affad1">那他在是怎么操作容器的呢？</span>
我们在创建容器的时候会自动创建一个pod，然后k8s会把我们真正要创建的这个容器自动给放到这个pod里面去，然后K8s在帮我们管理这个容器的时候，他并不会直接操作这个容器本身，他唯一能操作的就是这个pod 对于K8s来说他管理这个pod就相当于管理了里面的容器

K8s为什么要设计pod这个东西呢？<span style="background:#affad1">k8s集群搭建出来就是为了管理容器，为什么他不直接管理容器非要设计出一个pod出来间接的管理容器呢？</span>
从K8s软件设计的角度讲，市面上常用的容器引擎软件有很多(docker,container,portman等)，要是让k8s直接去管理底层的容器，这就意味着它需要去对接多种容器引擎软件，出一款新的软件，k8s就需要对接代码就要修改，从软件更新的角度来说就太繁琐了。所以K8S设计了一个自己能直接管理的东西，这个东西就是pod，容器创建出来之后，我全都给你放到pod里面，我只要能够操作这个pod就行，我不需要关心底层中，容器是拿那个软件建出来的

刚才说一个pod中放一个容器，但是实际k8s通过pod帮我们管理容器的时候，一个pod中会有两个容器
从思想上说这个容器的ip,容器的挂载的持久化呀都是挂载到这个pod上的，但是实际上pod就是一个破空壳子，他凭什么能够接收ip,能够挂载目录,能够发布服务呀？
其实我们在k8s里面创建一个业务容器，然后这个容器会被K8s扔到一个pod里面，这个时候呢K8s实际上会在这个pod里面自动的再创建一个容器！！！这个容器就是为了，能够通过dhcp获取ip挂载目录发布项目的，他自动创建这个容器用的镜像就是<span style="background:#affad1">pause</span>
![[Pasted image 20240725192828.png]]

#### 2.2 创建管理pod
创建pod就相当于创建容器
##### (1)创建pod
```bash title:创建pod
apiVersion: v1
kind: Pod
metadata:                                                         # 元数据
    name: test1-pod
    namespace: web                                      # 命名空间
spec:                                                                  # 容器信息
    containers:
        - name: test1-pod                            # 容器名
           image: centos:7
           imagePullPolicy: IfNotPresent      # 指定镜像的下载策略
           command:                                      #容器自动执行的命令
          - sleep
          - "3600"
```
kubectl create -f test1-pod.yaml
真正创建之前，在两个工作节点上导入镜像
##### (2)查看这个命令空间里面的pod
kubectl get pod -n web  
kubectl get pod -n web -o wide  # 查看更加详细的信息
##### (3)查看pod创建过程(可以用来排查错误)
kubectl describe pod test1-pod -n web
##### (4)查看日志
kubectl logs test1-pod -n web
##### (5)连接登录
kubectl exec -ti test1-pod -n web bash
##### (6)物理机pod间拷贝文件
kubectl cp file01 test1-pod:/file01 -n web
kubectl exec -ti test1-pod -n web ls /
![[Pasted image 20240725200107.png]]

#### 2.3 pod常用选项
![[Pasted image 20240725200737.png]]
##### (1)指定容器名称
name: xxxx
##### (2)指定镜像名称
image: xxxxx
##### (3)指定镜像下载策略
imagePullPolicy: [Always|IfNotPresent|Never]
	always:总是联网下载
	never:从不联网下再
	IfNotPresent:有就不下载没有就下载
##### (4)指定容器自动执行的命令
```bash
command:
- sleep
- "3600"
```
##### (5)指定命令的参数
```bash
command:
- sleep
args:
- "3600"
```
##### (6)容器中的服务端口
```bash
ports:
- containerPort: 80
```
##### (7)传递环境变量
```bash
env:
- name: 变量名称
   value: 值 
```
##### (8)资源限制
```bash
resources:
	requests:                         # 软限制
		cpu: "2000m"
		memory: "2G"
	limits:                               # 硬限制
		cpu: "4000m"          # m:毫核
		memory: "4G"
```
1000毫核 = 1核
##### (9)创建mysql容器
ctr -n k8s.io image import mysql57.tar
导入镜像
![[Pasted image 20240725203734.png]]
```bash
kubectl create -f pod-mysql.yaml
kubectl get pod -n web 
kubectl exec -ti mysql -n web bash
mysql -uroot -prehat
```
### 3.pod健康状态检测机制
#### 3.1 健康状态检测探针
做健康状态检查的两种方法
livenessprobe  
检测pod是否正常启动
readnessprobe  
检测pod是否能正常接收请求
#### 3.2 健康状态检测的方式
httpGet  
发送http请求，检测状态码200-400间，说明服务健康，否则不健康
![[Pasted image 20240725204813.png]]
tcpSocket  
针对所有tcp服务
6379redis端口
![[Pasted image 20240725205122.png]]
exec  
执行shell命令，判断命令的状态码
![[Pasted image 20240725205159.png]]

<span style="background:#affad1">健康状态检查加不加有影响吗？</span>
其实没啥影响，但是必须要加！！
在一些集群环境下，尤其是负载均衡状态下
 ![[Pasted image 20240725210546.png]]
 检测这些业务好不好，是通过负载均衡器检测业务是否正常运行
 如果没有健康状态检查，负载均衡器在接受请求的时候有可能把这个请求分发到故障的业务服务器上面，导致客户访问不正常，如果有健康状态检查，负载均衡器就不再往故障机器上分发请求
### 4.deployment无状态负载
#### 4.1 无状态负载介绍
一种特殊的pod
在k8s里面创建业务的时候，我们强烈不建议直接去构建pod
大多都是：要么创建无状态负载，要么创建有状态负载
直接去创建pod的话，K8s里面很多的高级功能pod不支持
比如：
自我修复
负载均衡
滚动更新
所以我们需要通过无状态负载来创建pod
优势：
<span style="background:#affad1">支持pod副本自动维护</span>
指定一模一样的容器有几个，有机器故障，副本数不够五个会自动创建直至副本数为5
<span style="background:#affad1">支持滚动更新</span>
<span style="background:#b1ffff">这些功能是怎么实现的呢？无状态负载往外建pod他的流程是怎样的？</span>
将来我们从K8s里面创建出一个无状态负载，它会自动创建出来三样东西
这个无状态负载本身会被创建
还会创建一个<span style="background:#ff4d4f">副本集</span>
副本集的作用：通过副本集帮我们真正的创建容器，通过副本集帮我们维护这些容器
最终副本集会创建出几个pod
![[Pasted image 20240725212127.png|286]]
k8s内部有很多控制器，然后经常被人提到的最著名的控制器是<span style="background:#affad1">副本集</span>
图上的三样东西加在一起称之为无状态负载
假设集群里面有两个无状态负载(两个副本集)
![[Pasted image 20240725212507.png|475]]
从K8s设计的角度来说，他怎么知道那个RS是用来管理那批容器的？
他是怎么在这个rs和实际的pod之间建立这个对应关系的？
K8s非常重要的一个概念：<span style="background:#ff4d4f">lable：标签选择器</span> ；一个标签就是一个键值对的数据
所以创建pod的时候要给pod分配标签 
例如：<span style="background:#affad1">app：web</span>
然后再在rs上面指定一个标签 <span style="background:#affad1">app：web</span>
这样子这个rs就回去维护标签为<span style="background:#affad1">app：web</span>的所有pod
包括滚动更新和副本的自动维护

k8s所有的资源他们之间建立关系的时候所有的资源都是靠标签来绑定的
<span style="background:#affad1">使用场景：频繁更新的业务</span>
#### 4.2 创建deployment
vim deployment.yaml
![[Pasted image 20240725214032.png]]
ctr -n k8s.io image import nginx16.tar
kubectl create -f deployment.yaml
![[Pasted image 20240725214808.png]]
##### (1)查看无状态负载
kubectl get deploy
![[Pasted image 20240725214856.png]]
##### (2)查看rs副本集
kubectl get rs
![[Pasted image 20240725215025.png]]
kubectl get pod
![[Pasted image 20240725215043.png]]
##### (3)查看pod标签
kubectl get pod --show-labels
![[Pasted image 20240725215058.png]]
##### (4)验证副本维护
kubectl delete pod  test1-nginx-5d858b7fc5-88j4t
![[Pasted image 20240725215256.png]]出现了个新的

#### 4.3 deployment滚动更新
还需要多加一些参数才能实现滚动更新
![[Pasted image 20240725215909.png]]
maxUnavailable: 3 超过三个更新失败，那么就认为整个更新都是失败的
kubectl create -f deployment.yaml
![[Pasted image 20240725220154.png]]
kubectl get pod
![[Pasted image 20240725220221.png]]

##### (1)测试滚动更新流程
每次更新的时候都会新建一个rs，他会连以前的rs都更新掉
只要有改动就叫更新！！！！
![[Pasted image 20240725220802.png]]
换成1.18做更新
##### (2)执行更新
kubectl apply -f  deployment.yaml
![[Pasted image 20240725220921.png]]
##### (3)查看更新过程
kubectl describe deploy test2-nginx
![[Pasted image 20240725221059.png]]
##### (4)查看更新历史
kubectl rollout history deploy test2-nginx
![[Pasted image 20240725221308.png]]

##### (5)版本回退
kubectl rollout undo deployment test2-nginx --to-revision=1
![[Pasted image 20240725221437.png]]
回退的过程也是一个滚动更新
##### (6)暂停更新
kubectl rollout pause deployment test2-nginx

## 六.kubernetes资源-其他特殊pod
### 1.daemonset服务集
简称:DS
也是一类特殊的pod
特征：
通过daemonset创建出来的pod数量是和工作节点一致的，如果五个工作节点就会创建五个pod，每个工作节点肯定会运行一个pod
应用场景：<span style="background:#affad1">便于部署客户端/agent工具。</span>例如zabbix-agent，filebeat
创建pod
kubectl delete -f web.yaml
vim daemonset
![[Pasted image 20240726092843.png]]
kubectl create -f daemonset 
kubectl get pod
![[Pasted image 20240726092926.png]]
一共两个节点一共创建出来两个pod
### 2.job
特殊的pod，类似于Linux系统中的一次性任务
典型的针对公司里面测试的工作(临时性的任务)
特征：pod在执行任务时，状态为running，任务执行完毕后，他的状态就会变为completed
应用场景：运行临时性任务(测试)
kubectl delete -f daemonset 
vim job.yaml
![[Pasted image 20240726094349.png]]
不设置pod重启策略，这个pod就成死循环了
<span style="background:#affad1">restartPolicy有三种</span>
Always: 总是(默认) 无论是正常退出还是异常退出
Never：重启不启动
OnFailure:  只有pod异常退出时，k8s才会重启他

 kubectl create -f job.yaml 
 ![[Pasted image 20240726095127.png]]
 但是状态是error
 kubectl logs 
![[Pasted image 20240726095648.png]]
日志这个提示(暂未解决)
最后发现是yaml文件写错了
![[Pasted image 20240726100035.png]]
$i; 应该这么写
![[Pasted image 20240726100110.png|575]]
成功解决！！！！
![[Pasted image 20240726100302.png]]
### 3.cronjob
笔记需要补充
特殊pod，类似于周期性计划任务
应用场景：执行重复行的任务(备份、清理日志、巡检脚本)
每执行一次就会创建一个新的pod(一定要设置限制)
**创建cronjob资源**
vim cronjob.yaml
![[Pasted image 20240726102417.png|650]]

![[Pasted image 20240726100645.png]]
过一会观察变化
![[Pasted image 20240726100745.png]]
旧的pod删除，有出现一个新的pod
```bash
successfulJobsHistoryLimit: 1 
failedJobsHistoryLimit: 1
```
它们定义了系统保留成功和失败作业历史记录的数量限制
因为cronjob每执行一次就会创建一个新的pod(一定要设置限制)
不限制的话存储空间会越来越少

有状态负载讲存储的时候在做补充

## 七.kubernetes资源-service服务
service相当于一个业务的访问入口
类似于负载均衡器的作用，不是单独用的，要配合着pod
假设我们有一个集群要跑一个网站，根据我们之前讲的就是创建一个无状态负载pod，然后把这个网站真正的部署起来
对于客户端而言，他最终带要能访问我们的网站，但是你<span style="background:#affad1">不能让客户端直接 访问这个pod，为什么呢？</span>
应为这个pod的ip也是k8s通过dhcp自动分配的，自动分配的说不准什么时候就会变<span style="background:#affad1">，这是第一个原因；第二个原因，</span>k8s针对无状态负载有自我修复的功能，pod异常退出了，k8s会帮我们重新创建这个pod,假设一共有两个工作节点，重新创建的pod不能确定会固定在那个节点上，所以对于客户端也没有办法直接通过pod去访问

假设我们创建了多个一样的pod搭建业务，多个一样的pod就是为了负载均衡，如果客户端能够直接访问pod的话，负载均衡就无法实现的，<span style="background:#affad1">pod本身是没有负载均衡功能的</span>
<span style="background:#d3f8b6">一是从负载均衡的角度来讲，二是从ip的角度来讲，客户端是无法直接访问pod的</span>
客户端是无法直接访问pod那k8s是怎么 设计的呢？
所以k8s设计了<span style="background:#ff4d4f">services</span>
将来我们在集群里面创建了一些pod，这些pod里面把业务真正部署起来之后，客户端怎么访问的呢？
k8s设计了<span style="background:#ff4d4f">services</span>（服务），
我们要想pod里面真正跑的网站的(80端口)让客户端能够访问到，我们要配合着pod在其前端建一个服务，这个service就相当于业务的入口，客户端访问service，再由service接收请求，然后把请求真正转到pod上去(有点类似于反向代理)
<span style="background:#b1ffff">任何一个实际跑在pod里面的业务都要对应这一个service</span>，pod和service从实际业务的角度来讲是配合着使用的

![[Pasted image 20240726190026.png]]
总结：业务的访问入口，类似于反向代理的作用；同时带有负载均衡的作用，但是负载均衡能不能实现，你需要看后端 pod的数量

某一个service接收到请求之后(他只负责接收请求)，他接收到请求之后，要把这个请求真正的转到后端的pod去处理，
<font color="#f79646">那他是怎么知道改吧请求转到那组pod上的呢？</font>
通过标签选择器在service和pod建立联系
![[Pasted image 20240726190351.png]]
<span style="background:#b1ffff">service简称svc</span>
创建K8s时候的初始化文件
![[Pasted image 20240726190651.png]]
其实k8s集群创建好之后就自带两个service
kubectl get svc -A
![[Pasted image 20240726190908.png]]
第一个服务就是真个集群创建的时候自动建的一个服务
作用：K8s搭好之后，我们管理员通过kubect向K8s发送操作请求的时候是由这个kube-apiserver接收的。
但是从服务的角度来说，我们通过kubect这个客户端命令干的所有的事情都是由kubernetes这个服务接收处理的。


ip 都会变 用主机名(域名)访问 docker里面好像也涉及到，翻翻回顾一下

服务的命名格式
支持简写，但是要分情况

